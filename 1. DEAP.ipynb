{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "372a1997-5ac2-45b8-bb47-dfa66f314f17",
   "metadata": {},
   "source": [
    "# Part 1. DEAP Dataset\n",
    "\n",
    "In this series of tutorial, we will be working on basic EEG analysis using python mne + pyTorch.  The case study will be on the DEAP dataset, a benchmark EEG emotion recognition dataset.   In this part 1, we will focus on looking at the dataset.\n",
    "\n",
    "This set of tutorial assumes:\n",
    "\n",
    "1.  You have already basic understanding of Python\n",
    "2.  You have some experience with scikit-learn, and also some knowledge about machine learning\n",
    "3.  You have a bit of experience with pyTorch and also some knowledge about deep learning\n",
    "\n",
    "In this dataset, there is a total of 32 participants, where each participant watches 40 1-minute videos.  Thus <code>s01.dat</code> is holding 40 batches.   The total sample is thus 40*32=1280 batches.\n",
    "\n",
    "Looking in each dat file (e.g., s01), it contains the data and label\n",
    "- Data ----- 40 x 40 x 8064 [\tvideo/batches x channel x samples ]\n",
    "- Label  ---- 40 x 4 \n",
    "\n",
    "Out of 40 channels, 32 channels were of EEG, and the rest of 8 of them from other sensors such as EOG (see the section 6.1 of the original paper).  We shall only extract the first 32 channels.   For the 8064, since the data is downsampled to 128Hz, thus one second contains around 128 samples, thus in one minute which is 60 seconds, it will be roughly 7680 samples.  The paper did not really talk a lot but it is likely there is  another 1.5 seconds before and after which total to 8064 samples (128 Hz * 63 seconds).\n",
    "\n",
    "The four labels correspond to valence, arousal, liking, and dominance, in this order.  We will only use valence and arousal, thus index 0 and 1 of the labels will be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3cee8c1-bc6a-4c95-80ea-671af1690efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4669e661-b903-44d2-b38c-b7a5d5f51e6a",
   "metadata": {},
   "source": [
    "Set cuda accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e458aa3-10e9-4190-9f7e-3b8cdcf462db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Configured device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb9bbd1-3efd-47e7-963e-a869b5cb85eb",
   "metadata": {},
   "source": [
    "## 1. Loading dataset\n",
    "\n",
    "Let's first create a simple dataset loader.   The code is explained using comments and is quite self-explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b7ba3a0-33e1-4078-bf22-31a7a80e8d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path, stim):\n",
    "        _, _, filenames = next(os.walk(path))\n",
    "        filenames = sorted(filenames)\n",
    "        all_data = []\n",
    "        all_label = []\n",
    "        for dat in filenames:\n",
    "            temp = pickle.load(open(os.path.join(path,dat), 'rb'), encoding='latin1')\n",
    "            all_data.append(temp['data'])\n",
    "            \n",
    "            if stim == \"Valence\":\n",
    "                all_label.append(temp['labels'][:,:1])   #the first index is valence\n",
    "            elif stim == \"Arousal\":\n",
    "                all_label.append(temp['labels'][:,1:2]) # Arousal  #the second index is arousal\n",
    "                \n",
    "        self.data = np.vstack(all_data)   #shape: (1280, 40, 8064) ==> 1280 samples / 40 samples = 32 participants\n",
    "        self.label = np.vstack(all_label) #(1280, )  ==> 1280 samples, each with a unique label (depend on the param \"stim\")\n",
    "        \n",
    "        del temp, all_data, all_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        single_data  = self.data[idx]\n",
    "        single_label = (self.label[idx] > 5).astype(float)   #convert the scale to either 0 or 1 (to classification problem)\n",
    "        \n",
    "        batch = {\n",
    "            'data': torch.Tensor(single_data),\n",
    "            'label': torch.Tensor(single_label)\n",
    "        }sss\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235d2fcc-2b8d-4609-b638-66e5ef79b186",
   "metadata": {},
   "source": [
    "Let's try load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39554964-e005-4402-a8ba-ad5cdaf02b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data\"  #create a folder \"data\", and inside put s01.dat,....,s32.dat inside from the preprocessed folder from the DEAP dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3e0dbec-b836-4ccf-a03c-540b097b5b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_valence = Dataset(path, \"Valence\")\n",
    "dataset_arousal = Dataset(path, \"Arousal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eba1cde-a5ce-46db-b4eb-a621a2f93a8d",
   "metadata": {},
   "source": [
    "We can try look at one sample using the index.  This is automatically mapped to the <code>__getitem__</code> function in the <code>Dataset</code> class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c534ec38-e9e3-4eb1-8f38-40ae83f0f7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': tensor([[ 9.4823e-01,  1.6533e+00,  3.0137e+00,  ..., -2.8265e+00,\n",
       "          -4.4772e+00, -3.6769e+00],\n",
       "         [ 1.2471e-01,  1.3901e+00,  1.8351e+00,  ..., -2.9870e+00,\n",
       "          -6.2878e+00, -4.4743e+00],\n",
       "         [-2.2165e+00,  2.2920e+00,  2.7464e+00,  ..., -2.6371e+00,\n",
       "          -7.4065e+00, -6.7559e+00],\n",
       "         ...,\n",
       "         [ 2.3078e+02,  6.9672e+02,  1.1951e+03,  ...,  1.0108e+03,\n",
       "           1.2831e+03,  1.5200e+03],\n",
       "         [-1.5418e+03, -1.6180e+03, -1.6927e+03,  ..., -1.5784e+04,\n",
       "          -1.5782e+04, -1.5781e+04],\n",
       "         [ 6.3905e-03,  6.3905e-03,  6.3905e-03,  ..., -9.7608e-02,\n",
       "          -9.7608e-02, -9.7608e-02]]),\n",
       " 'label': tensor([1.])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_valence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9eb8ba2-384d-422c-9009-91ae2f376a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data:  torch.Size([40, 8064])\n",
      "Shape of label:  torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of data: \", dataset_valence[0]['data'].shape)  #40 channels of data, 8064 samples in 1 minute\n",
    "print(\"Shape of label: \", dataset_valence[0]['label'].shape) #just 1 single label; 0 or 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea86680-c7bc-4876-9828-1d8da5bf2793",
   "metadata": {},
   "source": [
    "Let's try to look at our data and label distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc88a258-44dc-46fb-add9-85dd472c92b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset_valence[:]['data']\n",
    "label = dataset_valence[:]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f419cdae-c566-499b-916b-7b6706395f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1280, 40, 8064])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so we got 1280 trial (40 videos * 32 participants = 1280, each with 40 channels of data, each video contains 8064 EEG samples)\n",
    "data.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f2833a5-da04-4c64-a8b5-7fc46d042997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1280, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so we got 1280 labels, i.e., one label per video\n",
    "label.shape  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9b328e-b76b-4006-b059-64cb62cabb96",
   "metadata": {},
   "source": [
    "Let's count how many 0 and 1 in the valence dataset, to see if there is some imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "345890e5-9187-4d22-9f0c-b4e92db19965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels 1 in valence dataset:  708\n",
      "Labels 0 in valence dataset:  572\n"
     ]
    }
   ],
   "source": [
    "cond_1 = label == 1\n",
    "cond_0 = label == 0\n",
    "\n",
    "print(\"Labels 1 in valence dataset: \", len(label[cond_1]))\n",
    "print(\"Labels 0 in valence dataset: \", len(label[cond_0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffabe52-7bb9-4ec3-be07-821e44e042b5",
   "metadata": {},
   "source": [
    "Let's also count in the valence dataset, to see if there is some imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dab99dd6-a9e9-42f7-8fbc-47b5c2a9e796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels 1 in arousal dataset:  708\n",
      "Labels 0 in arousal dataset:  572\n"
     ]
    }
   ],
   "source": [
    "cond_1 = label == 1\n",
    "cond_0 = label == 0\n",
    "\n",
    "print(\"Labels 1 in arousal dataset: \", len(label[cond_1]))\n",
    "print(\"Labels 0 in arousal dataset: \", len(label[cond_0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca83cf33-8959-4c5b-85dd-7ec1a3ba126b",
   "metadata": {},
   "source": [
    "To confirm that the first 32 channels are EEG and the rest of the 8 channels are other channels, let's check the median value of each channel to see whether there is a pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "430a4f35-f63a-40d6-bb68-2ca6c83f462b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median of 0 data: 0.05827333778142929\n",
      "Median of 1 data: 0.024529436603188515\n",
      "Median of 2 data: -0.019204378128051758\n",
      "Median of 3 data: 0.033645644783973694\n",
      "Median of 4 data: -0.033030420541763306\n",
      "Median of 5 data: -0.016304221004247665\n",
      "Median of 6 data: -0.008036154322326183\n",
      "Median of 7 data: 0.09355251491069794\n",
      "Median of 8 data: -0.00792337954044342\n",
      "Median of 9 data: 0.021872472018003464\n",
      "Median of 10 data: 0.004741182550787926\n",
      "Median of 11 data: -0.02171526849269867\n",
      "Median of 12 data: -0.011923680081963539\n",
      "Median of 13 data: -0.04902170971035957\n",
      "Median of 14 data: -0.04108745604753494\n",
      "Median of 15 data: 0.033856555819511414\n",
      "Median of 16 data: 0.05146871879696846\n",
      "Median of 17 data: 0.03564863279461861\n",
      "Median of 18 data: -0.017957160249352455\n",
      "Median of 19 data: 0.007688858546316624\n",
      "Median of 20 data: 0.043062545359134674\n",
      "Median of 21 data: 0.019127536565065384\n",
      "Median of 22 data: -0.0017579937120899558\n",
      "Median of 23 data: -0.006185607053339481\n",
      "Median of 24 data: 0.015526460483670235\n",
      "Median of 25 data: 0.10526316612958908\n",
      "Median of 26 data: 0.003382256720215082\n",
      "Median of 27 data: -0.01977178454399109\n",
      "Median of 28 data: -0.012524685822427273\n",
      "Median of 29 data: 0.0033319643698632717\n",
      "Median of 30 data: -0.012974156066775322\n",
      "Median of 31 data: -0.02609466202557087\n",
      "Median of 32 data: 3.0773868560791016\n",
      "Median of 33 data: -14.912814140319824\n",
      "Median of 34 data: -3.8114349842071533\n",
      "Median of 35 data: 5.339752674102783\n",
      "Median of 36 data: 407.8304748535156\n",
      "Median of 37 data: -172.8857421875\n",
      "Median of 38 data: 70.57766723632812\n",
      "Median of 39 data: -0.01525240857154131\n"
     ]
    }
   ],
   "source": [
    "for i in range(40):\n",
    "    print(f\"Median of {i} data: {torch.median(data[:, i, :])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce38bbd-82f8-4e59-9ae3-e18d68151541",
   "metadata": {},
   "source": [
    "As we can see, the data index 0 to 31 is clearly EEG, while data from 32 onward is not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1108be-699d-4dbc-8488-d0fc076d448c",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "The way we process our dataset has two important problems we have to fix:\n",
    "1. First, make sure we only take 32 channels of EEG.  Of course, feel free to play around with other channels of data as well but this tutorial focuses on EEG.\n",
    "2. Since we got two labels, it is rather difficult for us to work on.  Let's divide them into four quadrants (similar to how the paper does, i.e., high-arousal-low-valence (label = 0), low arousal-low-valence, etc.\n",
    "\n",
    "Note that since the data is already preprocessed by the authors, we don't have to do anything more, but it's very natural for us to do preprocessing, e.g., min-max normalization, notch filters, band pass filters, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c094768-92df-4f06-b556-f2cd654770b5",
   "metadata": {},
   "source": [
    "## 2. Loading dataset (version 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "434c935d-47df-4165-9053-3022ecb44f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        _, _, filenames = next(os.walk(path))\n",
    "        filenames = sorted(filenames)\n",
    "        all_data = []\n",
    "        all_label = []\n",
    "        for dat in filenames:\n",
    "            \n",
    "            temp = pickle.load(open(os.path.join(path,dat), 'rb'), encoding='latin1')\n",
    "            all_data.append(temp['data'])\n",
    "            \n",
    "            #####divide labels into four classes: LALV, HALV, LAHV, HAHV\n",
    "            labels = temp['labels']\n",
    "            labels_holder = np.zeros((40, 1))\n",
    "            \n",
    "            val_med = np.median(labels[:, 0])\n",
    "            aro_med = np.median(labels[:, 1])\n",
    "\n",
    "            cond_lalv = (labels[:, 0] <= val_med) & (labels[:, 1] <= aro_med)\n",
    "            cond_halv = (labels[:, 0] <= val_med) & (labels[:, 1] >= aro_med)\n",
    "            cond_lahv = (labels[:, 0] >= val_med) & (labels[:, 1] <= aro_med)\n",
    "            cond_hahv = (labels[:, 0] >= val_med) & (labels[:, 1] >= aro_med)\n",
    "            \n",
    "            labels_holder[cond_lalv] = 0  #LALV\n",
    "            labels_holder[cond_halv] = 1  #HALV\n",
    "            labels_holder[cond_lahv] = 2  #LAHV\n",
    "            labels_holder[cond_hahv] = 3  #HAHV\n",
    "                                    \n",
    "            #labels_holder shape: (40, 1)\n",
    "            all_label.append(labels_holder)\n",
    "                \n",
    "        self.data = np.vstack(all_data)[:, :32, ]   #shape: (1280, 32, 8064) --> take only the first 32 channels\n",
    "        self.label = np.vstack(all_label) #(1280, 1)  ==> 1280 samples, \n",
    "        \n",
    "        del temp, all_data, all_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        single_data  = self.data[idx]\n",
    "        single_label = self.label[idx]\n",
    "        \n",
    "        batch = {\n",
    "            'data': torch.Tensor(single_data),\n",
    "            'label': torch.Tensor(single_label)\n",
    "        }\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab7a705-02ed-41d2-85f6-d4f6569ecd6f",
   "metadata": {},
   "source": [
    "Now let's try to load the dataset and see the shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5047cce-181e-4906-a2c1-915193f1391e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:  torch.Size([1280, 32, 8064])\n",
      "Label shape:  torch.Size([1280, 1])\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(path)\n",
    "\n",
    "data = dataset[:]['data']\n",
    "label = dataset[:]['label']\n",
    "\n",
    "print(\"Data shape: \", data.shape)\n",
    "print(\"Label shape: \", label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b6737f-e600-49c9-be70-5dcea31ecf3f",
   "metadata": {},
   "source": [
    "Let's look the label distribution of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "886493de-8f75-45cd-89ec-ebce9f23ac6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of LALV:  344\n",
      "count of HALV:  285\n",
      "count of LAHV:  281\n",
      "count of HAHV:  370\n"
     ]
    }
   ],
   "source": [
    "lalv = label == 0\n",
    "halv = label == 1\n",
    "lahv = label == 2\n",
    "hahv = label == 3\n",
    "\n",
    "assert len(label[lalv]) + len(label[halv]) + len(label[lahv]) + len(label[hahv]) == label.shape[0]  #simple unit test\n",
    "print(\"count of LALV: \", len(label[lalv]))\n",
    "print(\"count of HALV: \", len(label[halv]))\n",
    "print(\"count of LAHV: \", len(label[lahv]))\n",
    "print(\"count of HAHV: \", len(label[hahv]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4809c948-869a-4c65-9980-a447e453e0d5",
   "metadata": {},
   "source": [
    "Let's see the median of EEG of each group (you can do std on your own exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4102328-6401-4d74-9b90-9efd7ed8ce13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median of LALV 0.008621817\n",
      "Median of HALV 0.008139102\n",
      "Median of LAHV 0.0065687215\n",
      "Median of HAHV 0.0016776982\n"
     ]
    }
   ],
   "source": [
    "lalv_unsqueeze = lalv.squeeze()\n",
    "halv_unsqueeze = halv.squeeze()\n",
    "lahv_unsqueeze = lahv.squeeze()\n",
    "hahv_unsqueeze = hahv.squeeze()\n",
    "\n",
    "print(\"Median of LALV\", np.median(data[lalv_unsqueeze, :, :]))\n",
    "print(\"Median of HALV\", np.median(data[halv_unsqueeze, :, :]))\n",
    "print(\"Median of LAHV\", np.median(data[lahv_unsqueeze, :, :]))\n",
    "print(\"Median of HAHV\", np.median(data[hahv_unsqueeze, :, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3edcd77-d218-43e4-84b4-4c3e38a85fc7",
   "metadata": {},
   "source": [
    "Hmm....certainly, we can see some differences in voltage, which could be due to some peaks.  Anyhow, in the next tutorial, we shall look at power spectrum which could help us look at the power at different frequencies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
