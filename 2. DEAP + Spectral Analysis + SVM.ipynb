{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "372a1997-5ac2-45b8-bb47-dfa66f314f17",
   "metadata": {},
   "source": [
    "# Part 2. DEAP Dataset + Spectral Analysis + SVM\n",
    "\n",
    "In this part 2, we will focus on performing spectral analysis.  Spectral analysis here refers to the analysis of theta (4 - 8 Hz), alpha (8 - 12 Hz), beta (12 - 30 Hz), and gamma (30 - 64 Hz).   \n",
    "\n",
    "Spectral analysis is a very basic and must-do analysis for emotions/cognitions/resting state since it is a common knowledge with abundant evidence that our emotion/cognition change how our brain signals oscillate.  For example, when we are calm, alpha is relatively high, likewise, when we are attentive, beta is relatively high and alpha becomes relatively lower.\n",
    "\n",
    "In this part, we shall extract these powers.  Then we shall visualize it.  Lastly, let's try some simple SVM and Logistic Regression and see if these features are useful for predicting the four valence-arousal classes that we have obtained from Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3cee8c1-bc6a-4c95-80ea-671af1690efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4669e661-b903-44d2-b38c-b7a5d5f51e6a",
   "metadata": {},
   "source": [
    "Set cuda accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e458aa3-10e9-4190-9f7e-3b8cdcf462db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Configured device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb9bbd1-3efd-47e7-963e-a869b5cb85eb",
   "metadata": {},
   "source": [
    "## 1. Loading dataset\n",
    "\n",
    "Let's first reuse the dataset loader we have created in Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b7ba3a0-33e1-4078-bf22-31a7a80e8d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        _, _, filenames = next(os.walk(path))\n",
    "        filenames = sorted(filenames)\n",
    "        all_data = []\n",
    "        all_label = []\n",
    "        for dat in filenames:\n",
    "            \n",
    "            temp = pickle.load(open(os.path.join(path,dat), 'rb'), encoding='latin1')\n",
    "            all_data.append(temp['data'])\n",
    "            \n",
    "            #####divide labels into four classes: LALV, HALV, LAHV, HAHV\n",
    "            labels = temp['labels']\n",
    "            labels_holder = np.zeros((40, 1))\n",
    "            \n",
    "            val_med = np.median(labels[:, 0])\n",
    "            aro_med = np.median(labels[:, 1])\n",
    "\n",
    "            cond_lalv = (labels[:, 0] <= val_med) & (labels[:, 1] <= aro_med)\n",
    "            cond_halv = (labels[:, 0] <= val_med) & (labels[:, 1] >= aro_med)\n",
    "            cond_lahv = (labels[:, 0] >= val_med) & (labels[:, 1] <= aro_med)\n",
    "            cond_hahv = (labels[:, 0] >= val_med) & (labels[:, 1] >= aro_med)\n",
    "            \n",
    "            labels_holder[cond_lalv] = 1  #LALV\n",
    "            labels_holder[cond_halv] = 2  #HALV\n",
    "            labels_holder[cond_lahv] = 3  #LAHV\n",
    "            labels_holder[cond_hahv] = 4  #HAHV\n",
    "                                    \n",
    "            #labels_holder shape: (40, 1)\n",
    "            all_label.append(labels_holder)\n",
    "                \n",
    "        self.data = np.vstack(all_data)[:, :32, ]   #shape: (1280, 32, 8064) --> take only the first 32 channels\n",
    "        self.label = np.vstack(all_label) #(1280, 1)  ==> 1280 samples, \n",
    "        \n",
    "        del temp, all_data, all_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        single_data  = self.data[idx]\n",
    "        single_label = self.label[idx]\n",
    "        \n",
    "        batch = {\n",
    "            'data': torch.Tensor(single_data),\n",
    "            'label': torch.Tensor(single_label)\n",
    "        }\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235d2fcc-2b8d-4609-b638-66e5ef79b186",
   "metadata": {},
   "source": [
    "Let's try load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39554964-e005-4402-a8ba-ad5cdaf02b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data\"  #create a folder \"data\", and inside put s01.dat,....,s32.dat inside from the preprocessed folder from the DEAP dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5047cce-181e-4906-a2c1-915193f1391e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:  torch.Size([1280, 32, 8064])\n",
      "Label shape:  torch.Size([1280, 1])\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(path)\n",
    "\n",
    "data = dataset[:]['data']\n",
    "label = dataset[:]['label']\n",
    "\n",
    "print(\"Data shape: \", data.shape)  #1280 = 32 * 40 trials, 32 EEG channels, 8064 samples\n",
    "print(\"Label shape: \", label.shape)  #four classes of LALV, HALV, LAHV, HAHV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b6737f-e600-49c9-be70-5dcea31ecf3f",
   "metadata": {},
   "source": [
    "Let's look the label distribution of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "886493de-8f75-45cd-89ec-ebce9f23ac6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of LALV:  344\n",
      "count of HALV:  285\n",
      "count of LAHV:  281\n",
      "count of HAHV:  370\n"
     ]
    }
   ],
   "source": [
    "cond_0 = label == 0  #just to make sure we really don't have any 0\n",
    "lalv = label == 1\n",
    "halv = label == 2\n",
    "lahv = label == 3\n",
    "hahv = label == 4\n",
    "\n",
    "assert len(label[cond_0]) == 0  #simple unit test\n",
    "assert len(label[lalv]) + len(label[halv]) + len(label[lahv]) + len(label[hahv]) == label.shape[0]  #simple unit test\n",
    "print(\"count of LALV: \", len(label[lalv]))\n",
    "print(\"count of HALV: \", len(label[halv]))\n",
    "print(\"count of LAHV: \", len(label[lahv]))\n",
    "print(\"count of HAHV: \", len(label[hahv]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4809c948-869a-4c65-9980-a447e453e0d5",
   "metadata": {},
   "source": [
    "Let's see the median of EEG of each group (you can do std on your own exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4102328-6401-4d74-9b90-9efd7ed8ce13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median of LALV 0.008621817\n",
      "Median of HALV 0.008139102\n",
      "Median of LAHV 0.0065687215\n",
      "Median of HAHV 0.0016776982\n"
     ]
    }
   ],
   "source": [
    "lalv_unsqueeze = lalv.squeeze()\n",
    "halv_unsqueeze = halv.squeeze()\n",
    "lahv_unsqueeze = lahv.squeeze()\n",
    "hahv_unsqueeze = hahv.squeeze()\n",
    "\n",
    "print(\"Median of LALV\", np.median(data[lalv_unsqueeze, :, :]))\n",
    "print(\"Median of HALV\", np.median(data[halv_unsqueeze, :, :]))\n",
    "print(\"Median of LAHV\", np.median(data[lahv_unsqueeze, :, :]))\n",
    "print(\"Median of HAHV\", np.median(data[hahv_unsqueeze, :, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3edcd77-d218-43e4-84b4-4c3e38a85fc7",
   "metadata": {},
   "source": [
    "## 2. Spectral Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdfb1bd-9899-45cf-b557-7a9ac61c8c1e",
   "metadata": {},
   "source": [
    "## 3. Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de2ff83-e5e3-4d95-b57a-218485efdf02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
