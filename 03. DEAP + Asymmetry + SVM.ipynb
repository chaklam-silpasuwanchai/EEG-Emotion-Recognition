{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. DEAP Dataset + Asymmetry + SVM\n",
    "\n",
    "In this part 3, we will focus on feature engineering using asymmetry analysis.  Asymmetry analysis here refers to the analysis of imbalance between left and right symmetrical location.\n",
    "\n",
    "Asymmetry analysis is another very basic and must-do analysis for emotions/cognitions/resting state.\n",
    "\n",
    "In this part, we shall extract these asymmetries as features and then input these features into SVM and see if these features are useful for predicting the four valence-arousal classes that we have obtained from Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:  (15360, 32, 672)\n",
      "Label shape:  (15360,)\n"
     ]
    }
   ],
   "source": [
    "# this time I will load the Dataset class from `./components/dataset.py`\n",
    "from components.dataset import Dataset\n",
    "path = \"data\"  #create a folder \"data\", and inside put s01.dat,....,s32.dat inside from the preprocessed folder from the DEAP dataset\n",
    "dataset = Dataset(path, \"Valence\")\n",
    "\n",
    "data  = np.array(dataset[:]['data'])\n",
    "label = np.array(dataset[:]['label']).squeeze()\n",
    "\n",
    "print(\"Data shape: \" , data.shape)  #15360 = 32 * 40 trials * 12 segments, 32 EEG channels, 672 samples\n",
    "print(\"Label shape: \", label.shape)  #two classes of valence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Asymmetry Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d07e120",
   "metadata": {},
   "source": [
    "Before we jump into the analysis, let's us think what did we do in the previous part2.\n",
    "\n",
    "First, we extract PSD in `3` different ways.\n",
    "\n",
    "- way_1. X_65 => We see spectral of the entire head the same\n",
    "- way_2. X_5 => We kind-of summarize the X_65 into 5 different bands but still a capture of the entire haed\n",
    "- way_3. (_,n_channels * freq_bands) => We look at each band on each locations of the head (channels). This data is more spatial than the previous ones.\n",
    "\n",
    "Finally, we learnt that more spatial data causes a better classification accuracy.\n",
    "\n",
    "This **might be** that frequency bands at different location and time is related to emotion\n",
    "\n",
    "WOW~!! sugoi!!!\n",
    "\n",
    "But we did extract the features that way (_, n_channels * freq_bands), if what I have said is true then there is nothing else we could do. right?\n",
    "\n",
    "Wellllllllllll............. There is a way.\n",
    "\n",
    "That why you are reading this tutorial.\n",
    "\n",
    "\n",
    "INTRODUCING!!!!! \n",
    "\n",
    "### ASYMMETRY ANALYSIS!!!\n",
    "\n",
    "\n",
    "Okay, sorry. I quite playing now.\n",
    "\n",
    "The name sounds fancy but actually the idea is simple. If you are happy, Alpha band on the left side of the head is higher than the right. (This is not true because I can not remember which side is higher but you get the idea).\n",
    "\n",
    "To help SVM capture this behavior, we will explicitly include this information as the feature. (You know, traditional ML is not every good at correlating the feature. We have to help them.)\n",
    "\n",
    "You can compare front-back, left-right, up-down, Fp1-Oz, .... The possibility is endless. Make sure to review papers to scope down the idea Or just run all of them (if you have time).\n",
    "\n",
    "Okay, enough chitchat. Let's do this!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e7c00",
   "metadata": {},
   "source": [
    "Here is the channels and their index.\n",
    "\n",
    "- Channels: 32\n",
    "  1.\tFp1\t\n",
    "  2.\tAF3\n",
    "  3.\tF3\n",
    "  4.\tF7\n",
    "  5.\tFC5\n",
    "  6.\tFC1\n",
    "  7.\tC3\n",
    "  8.\tT7\n",
    "  9.\tCP5\n",
    "  10.\tCP1\n",
    "  11.\tP3\n",
    "  12.\tP7\n",
    "  13.\tPO3\n",
    "  14.\tO1\n",
    "  15.\tOz\n",
    "  16.\tPz\n",
    "  17.\tFp2\n",
    "  18.\tAF4\n",
    "  19.\tFz\n",
    "  20.\tF4\n",
    "  21.\tF8\n",
    "  22.\tFC6\n",
    "  23.\tFC2\n",
    "  24.\tCz\n",
    "  25.\tC4\n",
    "  26.\tT8\n",
    "  27.\tCP6\n",
    "  28.\tCP2\n",
    "  29.\tP4\n",
    "  30.\tP8\n",
    "  31.\tPO4\n",
    "  32.\tO2\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/6e/International_10-20_system_for_EEG-MCN.svg\">\n",
    "\n",
    "\n",
    "I will seperate the channels into left-ones and right-ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d287455a",
   "metadata": {},
   "source": [
    "Based on 10-20 system, the followed number if odd, is on the left (if even, on the right). `z` is middle.\n",
    "\n",
    "[EEG-Based Emotion Recognition Using Logistic Regression with Gaussian Kernel and Laplacian Prior and Investigation of Critical Frequency Bands](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwj7kei-1Mr2AhVTzDgGHdH_D3oQFnoECAkQAQ&url=https%3A%2F%2Fwww.mdpi.com%2F2076-3417%2F10%2F5%2F1619%2Fpdf&usg=AOvVaw3nwBT1NPFALmqKKm4rbIuE)\n",
    "\n",
    "Based on this paper, we there are 14 pairs of left-right of asymmetry we can engineer (fancy word for calculate).\n",
    "\n",
    "1. Fp1-Fp2\n",
    "2. F7-F8\n",
    "3. F3-F4\n",
    "4. T7-T8\n",
    "5. P7-P8\n",
    "6. C3-C4\n",
    "7. P3-P4\n",
    "8. O1-O2\n",
    "9. AF3-AF4\n",
    "10. FC5-FC6\n",
    "11. FC1-FC2\n",
    "12. CP5-CP6\n",
    "13. CP1-CP2\n",
    "14. PO3-PO4\n",
    "\n",
    "And there are two relation we can create. Differential Asymmetry (DASM) and Rational Asymmetry (RASM)\n",
    "\n",
    "Another 11 pairs of frontal-posterior is as follow\n",
    "\n",
    "1. FC5-CP5\n",
    "2. FC1-CP1\n",
    "3. FC2-CP2\n",
    "4. FC6-CP6\n",
    "5. F7-P7\n",
    "6. F3-P3\n",
    "7. Fz-Pz\n",
    "8. F4-P4\n",
    "9. F8-P8\n",
    "10. Fp1-O1\n",
    "11. Fp2-O2\n",
    "\n",
    "The paper uses differential caudality (DCAU)\n",
    "\n",
    "$$ DASM = DE(X_{left}) - DE(X_{right}) $$\n",
    "$$ RASM = \\frac{DE(X_{left})}{DE(X_{right})} $$\n",
    "$$ DCAU = DE(X_{frontal}) - DE(X_{posterior}) $$\n",
    "\n",
    "$ DE() $ is a function that convert PSD to log-PSD. (There is more about it please read the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba82ff50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left_channel_indexes=[0, 3, 2, 7, 11, 6, 10, 13, 1, 4, 5, 8, 9, 12]\n",
      "right_channel_indexes=[16, 20, 19, 25, 29, 24, 28, 31, 17, 21, 22, 26, 27, 30]\n",
      "frontal_channel_indexes=[4, 5, 22, 21, 3, 2, 18, 19, 20, 0, 16]\n",
      "posterior_channel_indexes=[8, 9, 27, 26, 11, 10, 15, 28, 29, 13, 31]\n"
     ]
    }
   ],
   "source": [
    "channels = ['Fp1','AF3','F3','F7','FC5','FC1','C3','T7','CP5','CP1','P3','P7','PO3','O1','Oz','Pz','Fp2','AF4','Fz','F4','F8','FC6','FC2','Cz','C4','T8','CP6','CP2','P4','P8','PO4','O2']\n",
    "left_channels = ['Fp1','F7','F3','T7','P7','C3','P3','O1','AF3','FC5','FC1','CP5','CP1','PO3']\n",
    "right_channels = ['Fp2','F8','F4','T8','P8','C4','P4','O2','AF4','FC6','FC2','CP6','CP2','PO4']\n",
    "left_channel_indexes = [ channels.index(ch) for ch in left_channels ]\n",
    "right_channel_indexes = [ channels.index(ch) for ch in right_channels ]\n",
    "\n",
    "print(f\"{left_channel_indexes=}\")\n",
    "print(f\"{right_channel_indexes=}\")\n",
    "\n",
    "frontal_channels = ['FC5','FC1','FC2','FC6','F7','F3','Fz','F4','F8','Fp1','Fp2']\n",
    "posterior_channels = ['CP5','CP1','CP2','CP6','P7','P3','Pz','P4','P8','O1','O2']\n",
    "\n",
    "frontal_channel_indexes = [ channels.index(ch) for ch in frontal_channels ]\n",
    "posterior_channel_indexes = [ channels.index(ch) for ch in posterior_channels ]\n",
    "\n",
    "print(f\"{frontal_channel_indexes=}\")\n",
    "print(f\"{posterior_channel_indexes=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f766d2c",
   "metadata": {},
   "source": [
    "Let's get PSD. I will use `MNE_feature` since it handles epochs-like data for me and I don't need to analyze the data (remember, the data is already preprocessed)\n",
    "\n",
    "The idea is as follow;\n",
    "\n",
    "1. I will slowly calculate PSD of each frequncy band.\n",
    "2. For each band, I will calculate DASM, RASM, and DCAU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79f316b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15360, 160) (15360, 70) (15360, 70) (15360, 55)\n"
     ]
    }
   ],
   "source": [
    "from mne_features.feature_extraction import FeatureExtractor\n",
    "\n",
    "bands = dict({\n",
    "    \"delta\": (0,4), \n",
    "    \"theta\": (4,8),\n",
    "    \"alpha\": (8,12),\n",
    "     \"beta\": (12,30),\n",
    "    \"gamma\": (30,64)\n",
    "})\n",
    "\n",
    "PSDs = []\n",
    "DASMs = []\n",
    "RASMs = []\n",
    "DCAUs = []\n",
    "for band_name,band_range in bands.items():\n",
    "    params = dict({\n",
    "        'pow_freq_bands__log':True,\n",
    "        'pow_freq_bands__normalize':False,\n",
    "        'pow_freq_bands__freq_bands':band_range\n",
    "    })\n",
    "    fe = FeatureExtractor(sfreq=128, selected_funcs=['pow_freq_bands'],params=params,n_jobs=8, memory=\"cache/\")\n",
    "    PSD = fe.fit_transform(X=data)\n",
    "    #      (15360, 32)\n",
    "    # print(PSD.shape)\n",
    "    \n",
    "\n",
    "    PSD_left = PSD[:, left_channel_indexes].copy()\n",
    "    PSD_right = PSD[:, right_channel_indexes].copy()\n",
    "    PSD_frontal = PSD[:, frontal_channel_indexes].copy()\n",
    "    PSD_posterior = PSD[:, posterior_channel_indexes].copy()\n",
    "    #       (15360, 14)     (15360, 14)      (15360, 11)        (15360, 11)\n",
    "    # print(PSD_left.shape, PSD_right.shape, PSD_frontal.shape, PSD_posterior.shape)\n",
    "    DASM = PSD_left - PSD_right\n",
    "    #      (15360, 14)\n",
    "    # print(DASM.shape)\n",
    "    RASM = PSD_left / PSD_right\n",
    "    #      (15360, 14)\n",
    "    # print(RASM.shape)\n",
    "    DCAU = PSD_frontal - PSD_posterior\n",
    "    #      (15360, 11)\n",
    "    # print(DCAU.shape)\n",
    "\n",
    "    # Will be use later for comparison\n",
    "    PSDs.append(PSD)\n",
    "    DASMs.append(DASM)\n",
    "    RASMs.append(RASM)\n",
    "    DCAUs.append(DCAU)\n",
    "\n",
    "PSDs = np.hstack(PSDs)\n",
    "DASMs = np.hstack(DASMs)\n",
    "RASMs = np.hstack(RASMs)\n",
    "DCAUs = np.hstack(DCAUs)\n",
    "\n",
    "#     (15360, 160) (15360, 70) (15360, 70) (15360, 55)\n",
    "print(PSDs.shape, DASMs.shape, RASMs.shape, DCAUs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15360,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = label.squeeze()\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa7cee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_ori,y_ori, kernel='rbf'):\n",
    "    # Make a copy because I am paranoid\n",
    "    X,y = X_ori.copy(), y_ori.copy()\n",
    "\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.utils import shuffle\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    X_shuff,y_shuff = shuffle(X,y)\n",
    "    model = SVC(kernel=kernel,max_iter=10000)\n",
    "    cross = cross_val_score(model, X_shuff, y_shuff, cv=3)\n",
    "\n",
    "    model = SVC(kernel=kernel, max_iter=10000)\n",
    "    model.fit(X_shuff, y_shuff)\n",
    "    ans = model.predict(X_shuff)\n",
    "    acc = sum(ans == y_shuff) / len(y_shuff)\n",
    "    return model, acc, cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd003d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tKernel=linear-PSDs| Acc=0.53633 | 3-CV score=0.53288 STD=0.02082| Time spend=58.18406867980957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tKernel=linear-DASMs| Acc=0.55469 | 3-CV score=0.55527 STD=0.01589| Time spend=32.90813899040222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tKernel=linear-RASMs| Acc=0.55358 | 3-CV score=0.51764 STD=0.04991| Time spend=1.9324543476104736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tKernel=linear-DCAUs| Acc=0.55332 | 3-CV score=0.56335 STD=0.00631| Time spend=33.437034368515015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tKernel=poly-PSDs| Acc=0.68848 | 3-CV score=0.65983 STD=0.04228| Time spend=67.38062953948975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tKernel=poly-DASMs| Acc=0.72682 | 3-CV score=0.68158 STD=0.01047| Time spend=35.642327547073364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tKernel=poly-RASMs| Acc=0.55365 | 3-CV score=0.55299 STD=9e-05| Time spend=38.68176770210266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tKernel=poly-DCAUs| Acc=0.73197 | 3-CV score=0.69564 STD=0.00779| Time spend=31.705512285232544\n",
      "\tKernel=rbf-PSDs| Acc=0.7043 | 3-CV score=0.67533 STD=0.00507| Time spend=117.91553449630737\n",
      "\tKernel=rbf-DASMs| Acc=0.73314 | 3-CV score=0.69824 STD=0.0039| Time spend=57.48490333557129\n",
      "\tKernel=rbf-RASMs| Acc=0.55358 | 3-CV score=0.55319 STD=9e-05| Time spend=68.62462854385376\n",
      "\tKernel=rbf-DCAUs| Acc=0.73151 | 3-CV score=0.69258 STD=0.00523| Time spend=52.47633194923401\n",
      "\tKernel=sigmoid-PSDs| Acc=0.48691 | 3-CV score=0.48711 STD=0.00172| Time spend=64.99256181716919\n",
      "\tKernel=sigmoid-DASMs| Acc=0.52083 | 3-CV score=0.51204 STD=0.00512| Time spend=43.00359034538269\n",
      "\tKernel=sigmoid-RASMs| Acc=0.55332 | 3-CV score=0.55267 STD=0.00109| Time spend=59.13428854942322\n",
      "\tKernel=sigmoid-DCAUs| Acc=0.51667 | 3-CV score=0.52572 STD=0.01493| Time spend=38.558884620666504\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "fe_name = ['PSDs', 'DASMs', 'RASMs', 'DCAUs']\n",
    "for kernel in ['linear','poly','rbf', 'sigmoid']:\n",
    "    for index, X in enumerate([PSDs, DASMs, RASMs, DCAUs]):\n",
    "        start = time.time()\n",
    "        model, acc, cross = train_model(X, y, kernel=kernel)\n",
    "        # We can save the model and reuse it later\n",
    "        print(f\"\\tKernel={kernel}-{fe_name[index]}| Acc={round(acc,5)} | 3-CV score={round(cross.mean(),5)} STD={round(cross.std(),5)}| Time spend={time.time() - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08f712c",
   "metadata": {},
   "source": [
    "Oh wow. The result is pretty good.\n",
    "\n",
    "Do you think if we use all of them, it will achieved more?\n",
    "\n",
    "Why don't we do it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "440aaac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15360, 355)\n"
     ]
    }
   ],
   "source": [
    "all_X = np.concatenate( [PSDs, DASMs, RASMs, DCAUs], axis=1  )\n",
    "print(all_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdb70f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tKernel=linear-all_X| Acc=0.55241 | 3-CV score=0.54941 STD=0.00967| Time spend=5.466062307357788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/root/projects/.venv/lib/python3.8/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tKernel=poly-all_X| Acc=0.44681 | 3-CV score=0.48236 STD=0.05018| Time spend=201.3437066078186\n",
      "\tKernel=rbf-all_X| Acc=0.55365 | 3-CV score=0.55319 STD=9e-05| Time spend=269.5489151477814\n",
      "\tKernel=sigmoid-all_X| Acc=0.55326 | 3-CV score=0.55384 STD=0.00088| Time spend=236.1786971092224\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for kernel in ['linear','poly','rbf', 'sigmoid']:\n",
    "        start = time.time()\n",
    "        model, acc, cross = train_model(all_X, y, kernel=kernel)\n",
    "        # We can save the model and reuse it later\n",
    "        print(f\"\\tKernel={kernel}-all_X| Acc={round(acc,5)} | 3-CV score={round(cross.mean(),5)} STD={round(cross.std(),5)}| Time spend={time.time() - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03444e81",
   "metadata": {},
   "source": [
    "Uh no.\n",
    "\n",
    "It takes longer time to train but achieved less than a smaller set of features.\n",
    "\n",
    "Confuse?\n",
    "\n",
    "Welcome to EEG world. Are you not entertained?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
