{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3eab870-2294-405a-b31d-9bfccb5ae4db",
   "metadata": {},
   "source": [
    "# Subject Dependent\n",
    "# Segment First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb04369f-de64-426d-8188-c125d9aa4dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold, ShuffleSplit\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from components.models import *\n",
    "from components.helper import *\n",
    "from components.dataset_jo import *\n",
    "from components.train import *\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "import random\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb83d369-eb48-4594-b692-c3080f30897d",
   "metadata": {},
   "source": [
    "## Training Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da7aef09-9a9d-4a19-ad62-58ed77881a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        LSTM\n",
    "        Conv1D_LSTM\n",
    "        Conv1D_LSTM_Attention\n",
    "        Conv1D_LSTM_SelfAttention\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        # set running mode : juypyter or py\n",
    "        # - jupyter = testing mode\n",
    "        # - py      = production mode\n",
    "        parser  = argparse.ArgumentParser()\n",
    "        parser.add_argument('-a', '--model_name',     help='model_name' , type=str, required=False)\n",
    "        parser.add_argument('-x', '--stim',           help='stim'       , type=int, required=False)\n",
    "        parser.add_argument('-s', '--segment_number', help='segment_number'    , type=int, required=False)\n",
    "        parser.add_argument('-l', '--len_reduction',  help='len_reduction' , type=str, required=False)\n",
    "        parser.add_argument('-f', '--isdebug',        help='Set running mode' , type=str, required=False)\n",
    "        args     = parser.parse_args()\n",
    "\n",
    "        if args.isdebug == 'yes' or 'json' in args.isdebug :\n",
    "            print(\"Jupyter mode\")\n",
    "            model_name     = 'LSTM'\n",
    "            stim           = 1\n",
    "            len_reduction  = 'mean'  # 'mean'  or 'sum' or 'last'\n",
    "            segment_number = 1 # 1, 3, 5, 60\n",
    "\n",
    "        else:\n",
    "            model_name     = str(args.model_name)\n",
    "            stim           = int(args.stim)\n",
    "            segment_number = int(args.segment_number)\n",
    "            len_reduction  = str(args.len_reduction)  # 'none' or 'mean' or 'sum' or 'last'\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "               \n",
    "        \n",
    "        ##============================================\n",
    "        #  !!!!!!!!!!!!     DO NOT EDIT BELOW\n",
    "        #============================================\n",
    "        \n",
    "        \n",
    "        self.device = get_freer_gpu()\n",
    "        # self.device = 'cpu'\n",
    "        \n",
    "\n",
    "        #========== Training Configurations==========\n",
    "        self.path = \"./data\" \n",
    "        \n",
    "        \n",
    "        # STIMULI_VALENCE = 0\n",
    "        # STIMULI_AROUSAL = 1       \n",
    "        self.stim      = stim\n",
    "        self.stim_name = 'AROUSAL' if self.stim else 'VALENCE'\n",
    "        self.segment_number   = segment_number\n",
    "\n",
    "        self.params     = {\"batch_size\" : 16, \"shuffle\" : True, \"pin_memory\" : True}\n",
    "        self.num_epochs = 30\n",
    "        self.lr         = 0.0001\n",
    "\n",
    "        # true only if using 'LSTM'\n",
    "        if model_name == 'LSTM' :\n",
    "            self.seq_len_first = True\n",
    "        else :\n",
    "            self.seq_len_first = False\n",
    "\n",
    "        self.debug = False\n",
    "        if self.debug:\n",
    "            self.num_epochs = 1\n",
    "            self.n_split    = 3\n",
    "\n",
    "        #========== Model Configurations==========\n",
    "        # model list \n",
    "\n",
    "        \n",
    "        \n",
    "        self.model_name    = model_name   # this should be match with the model class\n",
    "        self.input_dim     = 32   # we got 32 EEG channels\n",
    "        self.hidden_dim    = 256  # let's define hidden dim as 256\n",
    "        self.num_layers    = 2    # we gonna have two LSTM layers\n",
    "        self.output_dim    = 1    # we got 2 classes so we can output only 1 number, 0 for first class and 1 for another class\n",
    "        self.bidirectional = True # uses bidirectional LSTM\n",
    "        self.dropout       = 0.5  # setting dropout to 0.5\n",
    "\n",
    "        # for self attention\n",
    "        self.len_reduction = len_reduction\n",
    "\n",
    "        # for multi head attention\n",
    "        self.n_heads       = 8\n",
    "        self.d_k           = (self.hidden_dim * 2) // self.n_heads # (256 * 2) // 8\n",
    "        \n",
    "        if self.model_name == 'CNN2D' :\n",
    "            if self.segment_number == 1:\n",
    "                self.fc_shape = 237568\n",
    "            if self.segment_number == 3:\n",
    "                self.fc_shape = 73728\n",
    "            if self.segment_number == 5:\n",
    "                self.fc_shape = 40960\n",
    "            if self.segment_number == 60:\n",
    "                self.fc_shape = 2048\n",
    "        \n",
    "        \n",
    "        #========== save config ==========\n",
    "        self.segsplit      = 'dependent-seg'\n",
    "        self.output_path   = f'./output/{self.segsplit}_{int(60/self.segment_number)}s/'\n",
    "        if args.isdebug == 'yes' or 'json' in args.isdebug :\n",
    "            self.result_csv    = f'{self.output_path}tmp_{self.model_name}_result.csv'\n",
    "        else:\n",
    "            self.result_csv    = f'{self.output_path}{self.model_name}_result.csv'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef506e0a-ea2f-4e6b-8730-71856a931eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter mode\n",
      "device : cuda:3\n",
      "path : ./data\n",
      "stim : 1\n",
      "stim_name : AROUSAL\n",
      "segment_number : 1\n",
      "params : {'batch_size': 16, 'shuffle': True, 'pin_memory': True}\n",
      "num_epochs : 20\n",
      "lr : 0.0001\n",
      "seq_len_first : True\n",
      "debug : False\n",
      "model_name : LSTM\n",
      "input_dim : 32\n",
      "hidden_dim : 256\n",
      "num_layers : 2\n",
      "output_dim : 1\n",
      "bidirectional : True\n",
      "dropout : 0.5\n",
      "len_reduction : mean\n",
      "n_heads : 8\n",
      "d_k : 64\n",
      "segsplit : dependent-seg\n",
      "output_path : ./output/dependent-seg_60s/\n",
      "result_csv : ./output/dependent-seg_60s/tmp_LSTM_result.csv\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "print_cls_var( config )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c2fb70-7d34-4f9d-8e99-2c6a755c2624",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c582275-b1ec-4d0f-9fde-bc9117ce3aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model( config ):\n",
    "    \n",
    "    if config.model_name == 'LSTM' :\n",
    "        model = LSTM( config.input_dim, \n",
    "                     config.hidden_dim, \n",
    "                     config.num_layers, \n",
    "                     config.output_dim, \n",
    "                     config.bidirectional, \n",
    "                     config.dropout)\n",
    "        \n",
    "    elif config.model_name == 'Conv1D_LSTM' :\n",
    "        model = Conv1D_LSTM( config.input_dim, \n",
    "                            config.hidden_dim, \n",
    "                            config.num_layers, \n",
    "                            config.output_dim, \n",
    "                            config.bidirectional, \n",
    "                            config.dropout\n",
    "                           )\n",
    "    elif config.model_name == 'Conv1D_LSTM_Attention' :\n",
    "        model = Conv1D_LSTM_Attention ( config.input_dim, \n",
    "                                       config.hidden_dim, \n",
    "                                       config.num_layers, \n",
    "                                       config.output_dim, \n",
    "                                       config.bidirectional, \n",
    "                                       config.dropout\n",
    "                                      )\n",
    "\n",
    "    elif config.model_name == 'Conv1D_LSTM_SelfAttention' :\n",
    "        model = Conv1D_LSTM_SelfAttention( config.input_dim, \n",
    "                                  config.hidden_dim, \n",
    "                                  config.num_layers, \n",
    "                                  config.output_dim, \n",
    "                                  config.bidirectional, \n",
    "                                  config.dropout, \n",
    "                                  config.len_reduction   \n",
    "                                 )\n",
    "    elif config.model_name == 'Conv1D_LSTM_MultiHeadSelfAttention' :\n",
    "        model =Conv1D_LSTM_MultiHeadSelfAttention( config.input_dim, \n",
    "                                                  config.hidden_dim, \n",
    "                                                  config.num_layers, \n",
    "                                                  config.output_dim, \n",
    "                                                  config.bidirectional, \n",
    "                                                  config.dropout, \n",
    "                                                  config.len_reduction,\n",
    "                                                  config.n_heads,\n",
    "                                                  config.d_k\n",
    "                                                 )\n",
    "    elif config.model_name == 'CNN2D' :\n",
    "        model = CNN2D( config.input_dim, \n",
    "                       config.output_dim,\n",
    "                      config.fc_shape \n",
    "                      \n",
    "                    )\n",
    "    \n",
    "    \n",
    "    model = model.to(config.device)  \n",
    "    model.apply(initialize_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr) \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    \n",
    "    return model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b90c946-f9b4-44ff-940d-6d8e0e317a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model LSTM has 2,171,393 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model, _, _ = init_model( config )\n",
    "print(f'The model {type(model).__name__} has {count_parameters(model):,} trainable parameters')# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b98ed7c-c5e2-41a4-babe-464acec8aaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 32 files\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset_subjectDependent(config.path)\n",
    "dataset.set_segment(config.segment_number)\n",
    "\n",
    "filenames = dataset.get_file_list()\n",
    "filenames.sort()\n",
    "# print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "666a316d-8455-41f6-b438-bff439fec426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_dataloader(X_orig, y_orig, trainval_idxs, test_idxs, params):\n",
    "    \n",
    "#     train_num = int(len(trainval_idxs)*0.75)\n",
    "#     # print(train_num, len(trainval_idxs))\n",
    "    \n",
    "#     random.shuffle(trainval_idxs)\n",
    "#     train_idxs = trainval_idxs[:train_num]\n",
    "#     val_idxs   = trainval_idxs[train_num:]\n",
    "    \n",
    "#     assert [i for i in train_idxs if i in val_idxs] == []\n",
    "    \n",
    "#     X_train, X_val, X_test = X_orig[train_idxs], X_orig[val_idxs], X_orig[test_idxs]\n",
    "#     y_train, y_val, y_test = y_orig[train_idxs], y_orig[val_idxs], y_orig[test_idxs]\n",
    "\n",
    "#     train_dataset = TensorDataset(torch.tensor(X_train).float() , torch.tensor(y_train).float())\n",
    "#     val_dataset   = TensorDataset(torch.tensor(X_val).float()   , torch.tensor(y_val).float())\n",
    "#     test_dataset  = TensorDataset(torch.tensor(X_test).float()  , torch.tensor(y_test).float())\n",
    "#     del X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \n",
    "#     print(\"len(train_dataset)\", len(train_dataset))\n",
    "#     print(\"len(val_dataset)  \", len(val_dataset))\n",
    "#     print(\"len(test_dataset) \", len(test_dataset))\n",
    "\n",
    "#     train_loader = DataLoader(train_dataset, **params)\n",
    "#     val_loader   = DataLoader(val_dataset  , **params)\n",
    "#     test_loader  = DataLoader(test_dataset , **params)\n",
    "    \n",
    "#     # print(\"len(train_loader)\", len(train_loader))\n",
    "#     # print(\"len(val_loader)  \", len(val_loader))\n",
    "#     # print(\"len(test_loader) \", len(test_loader))\n",
    "    \n",
    "#     return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6f7c701-0cbe-41d0-8043-ce32ef68ac47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL Best Model from Best Epoch 19 Test Loss = 0.53005450963974, Test Acc = 0.75\n",
      "FINAL Best Model from Best Epoch 19 Test Loss = 0.5710050463676453, Test Acc = 0.75\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_555/125190536.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# === DO TRAINING ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             train_loss, valid_loss, train_acc , valid_acc , test_loss, test_acc, best_epoch, epoch_times = train(config.num_epochs,\n\u001b[0m\u001b[1;32m     52\u001b[0m                                                                                                      \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                                                                                                      \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/EEG-Emotion-Recognition/components/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, model, train_loader, val_loader, test_loader, optimizer, criterion, device, seq_len_first)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/EEG-Emotion-Recognition/components/train.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(model, train_loader, optimizer, criterion, device, seq_len_first)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m#predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#output shape: (batch, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mloss\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/EEG-Emotion-Recognition/components/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#x = [batch size, seq len, channels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#out = [batch size, seq len, hidden dim * num directions]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    692\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_test_acc = []\n",
    "\n",
    "for filename in filenames:\n",
    "    # print(\"==================================================\")\n",
    "    # print(\"Participant : \", filename )\n",
    "    \n",
    "    # get each participant dataset without the groups\n",
    "    if config.model_name == 'CNN2D':\n",
    "        print(\"Getting spectrogram data ... \")\n",
    "        X, y, _ = dataset.get_spec_data(filename, config.stim, return_type='numpy')\n",
    "    else : \n",
    "        X, y, _ = dataset.get_data(filename, config.stim, return_type='numpy')\n",
    "    # print(filename, X.shape, y.squeeze().shape)\n",
    "\n",
    "    # ----------- OUTER CROSS VALIDATION -----------\n",
    "    cv_outer = StratifiedKFold( n_splits = 10 )\n",
    "    X_orig, y_orig = X.copy(), y.copy()\n",
    "\n",
    "    for outer_fold, ( trainval_idxs, test_idxs ) in enumerate( cv_outer.split(X, y.squeeze())):\n",
    "\n",
    "        # print( \"Outer Fold : \", outer_fold )\n",
    "        # print( trainval_idxs.shape, test_idxs.shape )\n",
    "\n",
    "        X_trainval, X_test = X_orig[trainval_idxs], X_orig[test_idxs]\n",
    "        y_trainval, y_test = y_orig[trainval_idxs], y_orig[test_idxs]\n",
    "        \n",
    "        test_dataset  = TensorDataset(torch.tensor(X_test).float()  , torch.tensor(y_test).float())\n",
    "        test_loader   = DataLoader(test_dataset , **config.params)\n",
    "        \n",
    "        # ------------ INNER CROSS VALIDATION -------------\n",
    "        cv_inner = ShuffleSplit( n_splits = 1 , train_size = 0.8, random_state = 42 )\n",
    "        all_test_preds = []\n",
    "        \n",
    "        for inner_fold, ( train_idxs, val_idxs ) in enumerate( cv_inner.split(X_trainval, y_trainval.squeeze())):\n",
    "\n",
    "            # print( \"Inner Fold : \", inner_fold )\n",
    "            # print( train_idxs.shape, val_idxs.shape )\n",
    "\n",
    "            X_train, X_val = X_trainval[train_idxs], X_trainval[val_idxs]\n",
    "            y_train, y_val = y_trainval[train_idxs], y_trainval[val_idxs]\n",
    "\n",
    "            train_dataset = TensorDataset(torch.tensor(X_train).float() , torch.tensor(y_train).float())\n",
    "            val_dataset   = TensorDataset(torch.tensor(X_val).float()   , torch.tensor(y_val).float())\n",
    "            train_loader  = DataLoader(val_dataset  , **config.params)\n",
    "            val_loader    = DataLoader(test_dataset , **config.params)\n",
    "\n",
    "            # === Init MODEL ===\n",
    "            model, optimizer, criterion = init_model( config )\n",
    "\n",
    "            # === DO TRAINING === \n",
    "            train_loss, valid_loss, train_acc , valid_acc , test_loss, test_acc, best_epoch, epoch_times = train(config.num_epochs,\n",
    "                                                                                                     model,\n",
    "                                                                                                     train_loader,\n",
    "                                                                                                     val_loader,\n",
    "                                                                                                     test_loader,\n",
    "                                                                                                     optimizer,\n",
    "                                                                                                     criterion,\n",
    "                                                                                                     config.device,\n",
    "                                                                                                     config.seq_len_first)\n",
    "\n",
    "            all_test_acc.append(test_acc)\n",
    "            del model, optimizer, criterion, train_loader, val_loader\n",
    "\n",
    "        # save to csv at specific epoch\n",
    "            for epoch in range( best_epoch + 1 ) :\n",
    "                result_csv_dic               = {}\n",
    "                result_csv_dic['segment_number'] =  config.segment_number\n",
    "                result_csv_dic['len_reduction']  =  config.len_reduction\n",
    "                result_csv_dic['par']         =  filename\n",
    "                result_csv_dic['stim_name']   =  config.stim_name\n",
    "                result_csv_dic['fold']        =  outer_fold\n",
    "                result_csv_dic['epoch']       =  epoch\n",
    "                result_csv_dic['train_loss']  = train_loss[epoch]\n",
    "                result_csv_dic['valid_loss']  = valid_loss[epoch]\n",
    "                result_csv_dic['train_acc']   = train_acc[epoch]\n",
    "                result_csv_dic['valid_acc']   = valid_acc[epoch]\n",
    "                result_csv_dic['epoch_times'] =  epoch_times[epoch]\n",
    "\n",
    "                if epoch == best_epoch:\n",
    "                    result_csv_dic['test_loss'] = test_loss\n",
    "                    result_csv_dic['test_acc']  = test_acc\n",
    "                else:\n",
    "                    result_csv_dic['test_loss'] = ''\n",
    "                    result_csv_dic['test_acc']  = ''\n",
    "\n",
    "                save_result_csv( result_csv_dic, config.result_csv )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccdee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"segsplit      \", config.segsplit)\n",
    "print(\"model_name    \", config.model_name)\n",
    "print(\"stim_name     \", config.stim_name)\n",
    "print(\"segment size  \", int(60/config.segment_number), 's')\n",
    "\n",
    "print(\"all_test_acc : \", all_test_acc)\n",
    "print(\"AVG all_test_acc : \", np.mean(all_test_acc))\n",
    "print(\"SD  all_test_acc : \", np.std(all_test_acc))\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08b4a57-abd2-42ee-b172-3cee076e952c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc64ea-9aef-4ee3-8965-4c3b64e2e4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb4023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_num = 1 : (40, 32, 119, 65)\n",
    "# seg_num = 3 : (120, 32, 39, 65)\n",
    "# seg_num = 5 : (200, 32, 23, 65)\n",
    "# seg_num = 60 : (2400, 32, 31, 5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
