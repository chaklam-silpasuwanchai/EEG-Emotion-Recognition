{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3eab870-2294-405a-b31d-9bfccb5ae4db",
   "metadata": {},
   "source": [
    "# Main 10-Fold Cross-Validation Subject Dependent Split first then Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb04369f-de64-426d-8188-c125d9aa4dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "from components.helper import epoch_time, binary_accuracy, count_parameters, get_par_data, get_segmented_data \n",
    "from components.train import train, evaluate, initialize_weights\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def get_freer_gpu():\n",
    "    os.system('nvidia-smi -q -d Memory |grep -A4 GPU|grep Free >gpu_free')\n",
    "    memory_available = [int(x.split()[2]) for x in open('gpu_free', 'r').readlines()]\n",
    "    gpu = f'cuda:{np.argmax(memory_available)}'\n",
    "    if os.path.exists(\"gpu_free\"):\n",
    "        os.remove(\"gpu_free\")\n",
    "    else:\n",
    "          print(\"The file does not exist\") \n",
    "    return gpu\n",
    "\n",
    "device = get_freer_gpu()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e4232ec-561b-4c34-b8ee-441a9f9aee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "\n",
    "path = \"../data\" \n",
    "stim = \"Arousal\"\n",
    "\n",
    "par_list    = list(range(32))\n",
    "num_segment = 60\n",
    "n_split     = 10\n",
    "sss         = StratifiedShuffleSplit(n_splits = n_split, test_size = 0.25, random_state = 0)\n",
    "\n",
    "params     = {\"batch_size\" : 16, \"shuffle\" : True, \"pin_memory\" : True}\n",
    "num_epochs = 50\n",
    "lr         = 0.0001\n",
    "model_saved_name = None\n",
    "\n",
    "if debug:\n",
    "    par_list    = list(range(2))\n",
    "    num_epochs = 1\n",
    "    n_split    = 3\n",
    "    sss        = StratifiedShuffleSplit(n_splits = n_split, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4160972-b510-4d29-9f6d-494b8222b8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#note that these params are simply obtained from trial and error; I got no theory to back up why I use certain numbers here...\n",
    "input_dim     = 32 #we got 32 EEG channels\n",
    "hidden_dim    = 256 #let's define hidden dim as 256\n",
    "num_layers    = 2  #we gonna have two LSTM layers\n",
    "output_dim    = 1  #we got 2 classes so we can output only 1 number, 0 for first class and 1 for another class\n",
    "bidirectional = True  #uses bidirectional LSTM\n",
    "dropout       = 0.5  #setting dropout to 0.5\n",
    "seq_len_first = False\n",
    "\n",
    "## LSTM is the only model that requires seq_len_first = True\n",
    "# seq_len_first = True\n",
    "class LSTM(nn.Module):\n",
    "    '''\n",
    "    Expected Input Shape: (batch, seq_len, channels)\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * num_layers, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [batch size, seq len, channels]\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        \n",
    "        #out = [batch size, seq len, hidden dim * num directions]        \n",
    "        #hn = [num layers * num directions, batch size, hidden dim]\n",
    "        #cn = [num layers * num directions, batch size, hidden dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        #hn = [batch size, hidden dim * num directions]\n",
    "        \n",
    "        return self.fc(hn)\n",
    "    \n",
    "class Conv1D_LSTM(nn.Module):\n",
    "    '''\n",
    "    Expected Input Shape: (batch, channels, seq_len)  <==what conv1d wants\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, bidirectional, dropout):\n",
    "        super(Conv1D_LSTM, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(input_dim, input_dim, kernel_size=16, stride=1, padding=1)\n",
    "        self.norm = nn.BatchNorm1d(input_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * num_layers, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # conv1d expects (batch, channels, seq_len)\n",
    "        # should not try too big a kernel size, which could lead to too much information loss\n",
    "        x = F.max_pool1d(F.relu(self.norm(self.conv1d(x))), kernel_size=3)\n",
    "        x = F.max_pool1d(F.relu(self.norm(self.conv1d(x))), kernel_size=3)\n",
    "\n",
    "        # change to (batch, seq_len, channels) because lstm expects\n",
    "        x = x.permute(0,2,1)\n",
    "        \n",
    "        #x = [batch size, seq len, channels]\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        \n",
    "        #out = [batch size, seq len, hidden dim * num directions]        \n",
    "        #hn = [num layers * num directions, batch size, hidden dim]\n",
    "        #cn = [num layers * num directions, batch size, hidden dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        #hn = [batch size, hidden dim * num directions]\n",
    "        \n",
    "        return self.fc(hn)\n",
    "\n",
    "######################################################################################################\n",
    "    \n",
    "class Conv1D_LSTM_Attention(nn.Module):\n",
    "    '''\n",
    "    Expected Input Shape: (batch, channels, seq_len)  <==what conv1d wants\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, bidirectional, dropout):\n",
    "        super(Conv1D_LSTM_Attention, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(input_dim, input_dim, kernel_size=16, stride=1, padding=1)\n",
    "        self.norm = nn.BatchNorm1d(input_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * num_layers, output_dim)\n",
    "    \n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        hidden = final_state.unsqueeze(2)  # hidden : [batch_size, n_hidden * num_directions(=2), 1(=n_layer)]\n",
    "        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2) # attn_weights : [batch_size, seq_len, 1]\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        # context = [batch_size, n_hidden * num_directions(=2), seq_len] * [batch_size, seq_len, 1] = [batch_size, n_hidden * num_directions(=2), 1]\n",
    "        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        return context, soft_attn_weights.cpu().data.numpy() # context : [batch_size, n_hidden * num_directions(=2)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # conv1d expects (batch, channels, seq_len)\n",
    "        # should not try too many conv1d or there is nothing to attend to, thus I have only left with one layer of conv1d\n",
    "        x = F.max_pool1d(F.relu(self.norm(self.conv1d(x))), kernel_size=3)\n",
    "\n",
    "        # change to (batch, seq_len, channels) because lstm expects\n",
    "        x = x.permute(0,2,1)\n",
    "        \n",
    "        #x = [batch size, seq len, channels]\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        \n",
    "        #out = [batch size, seq len, hidden dim * num directions]        \n",
    "        #hn = [num layers * num directions, batch size, hidden dim]\n",
    "        #cn = [num layers * num directions, batch size, hidden dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        #hn = [batch size, hidden dim * num directions]\n",
    "        \n",
    "        attn_output, attention = self.attention_net(out, hn)\n",
    "\n",
    "        return self.fc(attn_output)\n",
    "\n",
    "######################################################################################################\n",
    "len_reduction = 'mean' # 'sum' or 'last'\n",
    "\n",
    "class Conv1D_LSTM_SelfAttention(nn.Module):\n",
    "    '''\n",
    "    Expected Input Shape: (batch, channels, seq_len)  <==what conv1d wants\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, bidirectional, dropout, len_reduction='mean'):\n",
    "        super(Conv1D_LSTM_SelfAttention, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(input_dim, input_dim, kernel_size=16, stride=1, padding=1)\n",
    "        self.norm = nn.BatchNorm1d(input_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * num_layers, output_dim)\n",
    "        self.lin_Q = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.lin_K = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.lin_V = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.len_reduction = len_reduction\n",
    "    \n",
    "    # lstm_output : [batch_size, seq len, n_hidden * num_directions(=2)]\n",
    "    def self_attention_net(self, lstm_output):\n",
    "        q = self.lin_Q(torch.clone(lstm_output))\n",
    "        k = self.lin_K(torch.clone(lstm_output))\n",
    "        v = self.lin_V(torch.clone(lstm_output))\n",
    "        # q : [batch_size, seq_len, n_hidden * num_directions(=2)]\n",
    "        # k.transpose(1, 2): [batch_size, n_hidden * num_directions(=2), seq_len]\n",
    "        # attn_w = [batch_size, seq_len, seq_len]\n",
    "                \n",
    "        attn_w = torch.matmul(q, k.transpose(1, 2))\n",
    "        sfmx_attn_w = F.softmax(attn_w, 1)\n",
    "        \n",
    "        # context = [batch_size, seq_len, hidden_dim * num_directions(=2)]\n",
    "        context = torch.matmul(sfmx_attn_w, v)\n",
    "        \n",
    "        # by doing some mean/sum, the dimension on the seq len is gone\n",
    "        if self.len_reduction == \"mean\":\n",
    "            return torch.mean(context, dim=1), sfmx_attn_w.cpu().data.numpy()\n",
    "        elif self.len_reduction == \"sum\":\n",
    "            return torch.sum(context, dim=1), sfmx_attn_w.cpu().data.numpy()\n",
    "        elif self.len_reduction == \"last\":\n",
    "            return context[:, -1, :], sfmx_attn_w.cpu().data.numpy()   \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # conv1d expects (batch, channels, seq_len)\n",
    "        # should not try too big a kernel size, which could lead to too much information loss\n",
    "        x = F.max_pool1d(F.relu(self.norm(self.conv1d(x))), kernel_size=3)\n",
    "\n",
    "        # change to (batch, seq_len, channels) because lstm expects\n",
    "        x = x.permute(0,2,1)\n",
    "        \n",
    "        #x = [batch size, seq len, channels]\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        \n",
    "        #out = [batch size, seq len, hidden dim * num directions]        \n",
    "        #hn = [num layers * num directions, batch size, hidden dim]\n",
    "        #cn = [num layers * num directions, batch size, hidden dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        #hn = [batch size, hidden dim * num directions]\n",
    "        \n",
    "        attn_output, attention = self.self_attention_net(out)\n",
    "\n",
    "        return self.fc(attn_output)\n",
    "    \n",
    "###################################################################################################\n",
    "n_heads       = 8   #<=======new!\n",
    "d_k           = (hidden_dim * 2) // n_heads # (256 * 2) // 8\n",
    "len_reduction = 'mean'  # 'sum' or 'last'\n",
    "\n",
    "class Conv1D_LSTM_SelfMultiHeadAttention(nn.Module):\n",
    "    '''\n",
    "    Expected Input Shape: (batch, channels, seq_len)  <==what conv1d wants\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, bidirectional, dropout, len_reduction='mean'):\n",
    "        super(Conv1D_LSTM_SelfMultiHeadAttention, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(input_dim, input_dim, kernel_size=16, stride=1, padding=1)\n",
    "        self.norm = nn.BatchNorm1d(input_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.softmax       = nn.LogSoftmax(dim=1)\n",
    "        self.fc = nn.Linear(hidden_dim * num_layers, output_dim)\n",
    "        self.lin_Q = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.lin_K = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.lin_V = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.len_reduction = len_reduction\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
    "    \n",
    "    # lstm_output : [batch_size, seq len, n_hidden * num_directions(=2)]\n",
    "    def self_multihead_attention_net(self, lstm_output):\n",
    "        \n",
    "        residual, batch_size = lstm_output, lstm_output.size(0) #<---residual added to the last output; batch_size may not be even for the last unit\n",
    "        \n",
    "        q = self.lin_Q(torch.clone(lstm_output))\n",
    "        k = self.lin_K(torch.clone(lstm_output))\n",
    "        v = self.lin_V(torch.clone(lstm_output))\n",
    "        # q : [batch_size, seq_len, n_hidden * num_directions(=2)]\n",
    "        # k.transpose(1, 2): [batch_size, n_hidden * num_directions(=2), seq_len]\n",
    "        # attn_w = [batch_size, seq_len, seq_len]\n",
    "        \n",
    "        #split into heads\n",
    "        q = q.view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q: [batch_size x n_heads x seq_len x d_k]\n",
    "        k = k.view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k: [batch_size x n_heads x seq_len x d_k]\n",
    "        v = v.view(batch_size, -1, n_heads, d_k).transpose(1,2)  # v: [batch_size x n_heads x seq_len x d_k]\n",
    "        \n",
    "        # dot production attention\n",
    "        attn_w = torch.matmul(q, k.transpose(-1, -2)) / np.sqrt(d_k) # [batch_size x n_heads x seq_len x seq_len]\n",
    "                \n",
    "        sfmx_attn_w = self.softmax(attn_w)\n",
    "        context = torch.matmul(sfmx_attn_w, v) # [batch_size x n_heads x seq_len x d_k]\n",
    "        \n",
    "        # concatenate heads\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_k) # context: [batch_size x seq_len x n_heads * d_k]\n",
    "        # now context: [batch_size, seq_len, hidden_dim * 2]\n",
    "        \n",
    "        # doing skip connection\n",
    "        # https://stats.stackexchange.com/questions/474440/why-do-transformers-use-layer-norm-instead-of-batch-norm\n",
    "        context = self.layer_norm(residual + context)\n",
    "\n",
    "        if self.len_reduction == \"mean\":\n",
    "            return torch.mean(context, dim=1), sfmx_attn_w.cpu().data.numpy()\n",
    "        elif self.len_reduction == \"sum\":\n",
    "            return torch.sum(context, dim=1), sfmx_attn_w.cpu().data.numpy()\n",
    "        elif self.len_reduction == \"last\":\n",
    "            return context[:, -1, :], sfmx_attn_w.cpu().data.numpy()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # conv1d expects (batch, channels, seq_len)\n",
    "        # should not try too big a kernel size, which could lead to too much information loss\n",
    "        x = F.max_pool1d(F.relu(self.norm(self.conv1d(x))), kernel_size=3)\n",
    "\n",
    "        # change to (batch, seq_len, channels) because lstm expects\n",
    "        x = x.permute(0,2,1)\n",
    "        \n",
    "        #x = [batch size, seq len, channels]\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        \n",
    "        #out = [batch size, seq len, hidden dim * num directions]        \n",
    "        #hn = [num layers * num directions, batch size, hidden dim]\n",
    "        #cn = [num layers * num directions, batch size, hidden dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        #hn = [batch size, hidden dim * num directions]\n",
    "        \n",
    "        attn_output, attention = self.self_multihead_attention_net(out)\n",
    "\n",
    "        return self.fc(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7713d60e-1116-4d55-855d-ffb8fdbdf3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model LSTM has 2,171,393 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(input_dim, hidden_dim, num_layers, output_dim, bidirectional, dropout)\n",
    "model = model.to(device)  \n",
    "model.apply(initialize_weights)\n",
    "print(f'The model {type(model).__name__} has {count_parameters(model):,} trainable parameters')# Train the model\n",
    "\n",
    "seq_len_first = True\n",
    "\n",
    "def reset_model():\n",
    "    model = LSTM(input_dim, hidden_dim, num_layers, output_dim, bidirectional, dropout)\n",
    "    model = model.to(device)  \n",
    "    model.apply(initialize_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    return model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ec93d13-7de0-4937-864c-fce96bcccdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Par :  s01.dat\n",
      "Training Par : 0 | Fold 0\n",
      "Best Epoch : 01 | Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.684 | Train Acc: 58.08%\n",
      "\t Val. Loss: 0.673  |  Val. Acc: 60.20%\n",
      "Training Par : 0 | Fold 1\n",
      "Best Epoch : 01 | Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.681 | Train Acc: 58.52%\n",
      "\t Val. Loss: 0.677  |  Val. Acc: 59.70%\n",
      "Training Par : 0 | Fold 2\n",
      "Best Epoch : 01 | Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.683 | Train Acc: 58.52%\n",
      "\t Val. Loss: 0.687  |  Val. Acc: 57.57%\n",
      "Par 0 train_loss = 0.6825183825155275\n",
      "Par 0 train_acc = 0.6788834260221114\n",
      "Par 0 valid_loss = 0.5837020648967551\n",
      "Par 0 valid_acc = 0.5915570175438596\n",
      "Par :  s02.dat\n",
      "Training Par : 1 | Fold 0\n",
      "Best Epoch : 01 | Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.673 | Train Acc: 59.40%\n",
      "\t Val. Loss: 0.666  |  Val. Acc: 60.03%\n",
      "Training Par : 1 | Fold 1\n",
      "Best Epoch : 01 | Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.680 | Train Acc: 58.96%\n",
      "\t Val. Loss: 0.657  |  Val. Acc: 60.20%\n",
      "Training Par : 1 | Fold 2\n",
      "Best Epoch : 01 | Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.670 | Train Acc: 59.13%\n",
      "\t Val. Loss: 0.705  |  Val. Acc: 58.55%\n",
      "Par 1 train_loss = 0.6742207801799155\n",
      "Par 1 train_acc = 0.6757399865932632\n",
      "Par 1 valid_loss = 0.5916297935103244\n",
      "Par 1 valid_acc = 0.5959429824561403\n",
      "Par :  s03.dat\n",
      "Training Par : 2 | Fold 0\n",
      "Best Epoch : 01 | Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.525 | Train Acc: 79.54%\n",
      "\t Val. Loss: 0.487  |  Val. Acc: 80.10%\n",
      "Training Par : 2 | Fold 1\n",
      "Best Epoch : 01 | Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.517 | Train Acc: 78.76%\n",
      "\t Val. Loss: 0.485  |  Val. Acc: 80.10%\n",
      "Training Par : 2 | Fold 2\n",
      "Best Epoch : 01 | Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.527 | Train Acc: 79.37%\n",
      "\t Val. Loss: 0.496  |  Val. Acc: 79.93%\n",
      "Par 2 train_loss = 0.5231797176915988\n",
      "Par 2 train_acc = 0.4891698164375205\n",
      "Par 2 valid_loss = 0.7922197640117994\n",
      "Par 2 valid_acc = 0.8004385964912281\n",
      "Par :  s04.dat\n",
      "Training Par : 3 | Fold 0\n",
      "Best Epoch : 01 | Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.672 | Train Acc: 58.96%\n",
      "\t Val. Loss: 0.656  |  Val. Acc: 61.18%\n",
      "Training Par : 3 | Fold 1\n",
      "Best Epoch : 01 | Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.663 | Train Acc: 60.95%\n",
      "\t Val. Loss: 0.687  |  Val. Acc: 55.92%\n",
      "Training Par : 3 | Fold 2\n",
      "Best Epoch : 01 | Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.670 | Train Acc: 58.85%\n",
      "\t Val. Loss: 0.655  |  Val. Acc: 61.18%\n",
      "Par 3 train_loss = 0.6683197912916673\n",
      "Par 3 train_acc = 0.665788139167585\n",
      "Par 3 valid_loss = 0.5958702064896756\n",
      "Par 3 valid_acc = 0.5942982456140351\n",
      "Par :  s05.dat\n",
      "Training Par : 4 | Fold 0\n",
      "Best Epoch : 01 | Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.693 | Train Acc: 53.04%\n",
      "\t Val. Loss: 0.692  |  Val. Acc: 51.15%\n",
      "Training Par : 4 | Fold 1\n",
      "Best Epoch : 01 | Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.695 | Train Acc: 52.93%\n",
      "\t Val. Loss: 0.700  |  Val. Acc: 50.66%\n",
      "Training Par : 4 | Fold 2\n",
      "Best Epoch : 01 | Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.698 | Train Acc: 50.33%\n",
      "\t Val. Loss: 0.700  |  Val. Acc: 50.00%\n",
      "Par 4 train_loss = 0.6953117958212321\n",
      "Par 4 train_acc = 0.6972615279649433\n",
      "Par 4 valid_loss = 0.5210176991150443\n",
      "Par 4 valid_acc = 0.506030701754386\n",
      "Par :  s06.dat\n",
      "Training Par : 5 | Fold 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8924a27fbc34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# === DO TRAINING ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         train_loss, train_acc, valid_loss, valid_acc = train(num_epochs,\n\u001b[0m\u001b[1;32m     38\u001b[0m                                                              \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                                                              \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_work/EEG-Emotion-Recognition/projects/components/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, model, train_loader, val_loader, optimizer, criterion, model_name, device, seq_len_first)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_work/EEG-Emotion-Recognition/projects/components/train.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(model, train_loader, optimizer, criterion, device, seq_len_first)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m#predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#output shape: (batch, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/archive/gpu/home/users/jakrapop.a/.conda/envs/jakrapop_nlu/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1fd957b48f87>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#x = [batch size, seq len, channels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m#out = [batch size, seq len, hidden dim * num directions]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/archive/gpu/home/users/jakrapop.a/.conda/envs/jakrapop_nlu/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/archive/gpu/home/users/jakrapop.a/.conda/envs/jakrapop_nlu/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    692\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result_dict = {}\n",
    "\n",
    "for par in par_list:\n",
    "    \n",
    "    result_dict[par] = {}\n",
    "    \n",
    "    # ==== GET ALL DATA OF THIS PAR ====\n",
    "    all_data, all_label = get_par_data(path, par, stim)\n",
    "\n",
    "    # ==== GET CV DATA OF THAT PARTICIPANT FOR EACH FOLD ====\n",
    "    for i_fold, (train_index, test_index) in enumerate(sss.split(all_data, all_label)):\n",
    "        \n",
    "        result_dict[par][i_fold] = {}\n",
    "        \n",
    "        print(f\"Training Par : {par} | Fold {i_fold}\")\n",
    "\n",
    "        X_train, X_test = all_data[train_index]  , all_data[test_index]\n",
    "        y_train, y_test = all_label[train_index] , all_label[test_index]\n",
    "\n",
    "        # === PERFORM SEGMENTATION on TRAIN and TEST set === \n",
    "\n",
    "        train_data, train_label = get_segmented_data(X_train, y_train, num_segment)\n",
    "        test_data,  test_label  = get_segmented_data(X_test,  y_test, num_segment)\n",
    "        del  X_train, X_test, y_train, y_test\n",
    "\n",
    "        train_dataset = TensorDataset(torch.tensor(train_data).float(), torch.tensor(train_label).float())\n",
    "        test_dataset  = TensorDataset(torch.tensor(test_data).float(), torch.tensor(test_label).float())\n",
    "        del train_data, train_label, test_data, test_label\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, **params)\n",
    "        val_loader  = DataLoader(test_dataset, **params)\n",
    "\n",
    "        # === RESET MODEL ===\n",
    "        model, optimizer, criterion = reset_model()\n",
    "        \n",
    "        # === DO TRAINING === \n",
    "        train_loss, train_acc, valid_loss, valid_acc = train(num_epochs,\n",
    "                                                             model,\n",
    "                                                             train_loader,\n",
    "                                                             val_loader,\n",
    "                                                             optimizer,\n",
    "                                                             criterion, model_saved_name,\n",
    "                                                             device,\n",
    "                                                             seq_len_first=seq_len_first)\n",
    "        result_dict[par][i_fold]['train_loss'] = train_loss\n",
    "        result_dict[par][i_fold]['train_acc']  = train_acc\n",
    "        result_dict[par][i_fold]['valid_loss'] = valid_loss\n",
    "        result_dict[par][i_fold]['valid_acc']  = valid_acc\n",
    "        \n",
    "        del model, optimizer, criterion, train_loader, val_loader\n",
    "\n",
    "    par_train_loss = [result_dict[par][i]['train_loss'][-1] for i in range(n_split)]\n",
    "    par_train_acc = [result_dict[par][i]['train_acc'][-1]   for i in range(n_split)]\n",
    "    par_valid_loss = [result_dict[par][i]['valid_loss'][-1] for i in range(n_split)]\n",
    "    par_valid_acc = [result_dict[par][i]['valid_acc'][-1]   for i in range(n_split)]\n",
    "    \n",
    "    print(f\"Par {par} AVG train_loss = {np.mean(par_train_loss)}\")\n",
    "    print(f\"Par {par} AVG train_acc = {np.mean(par_train_acc)}\")    \n",
    "    print(f\"Par {par} AVG valid_loss = {np.mean(par_valid_loss)}\")    \n",
    "    print(f\"Par {par} AVG valid_acc = {np.mean(par_valid_acc)}\")    \n",
    "    \n",
    "with open(f'./models/DEAP_LSTM_{stim}_results.pkl', 'wb') as outp:\n",
    "    pickle.dump(result_dict, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9002a9-aa07-41f9-bc6a-4ce02c0fe61a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b6dcbf-ced8-47fb-bfc7-aa647cf22c90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jakrapop_nlu",
   "language": "python",
   "name": "jakrapop_nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
