{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3eab870-2294-405a-b31d-9bfccb5ae4db",
   "metadata": {},
   "source": [
    "# Main 10-Fold Cross-Validation Subject Dependent Split first then Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb04369f-de64-426d-8188-c125d9aa4dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import time\n",
    "from components.helper import epoch_time, binary_accuracy, count_parameters, get_par_data, get_segmented_data \n",
    "from components.train import train, evaluate, initialize_weights\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = 'cuda:0'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e4232ec-561b-4c34-b8ee-441a9f9aee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_saved_name = \"models/DEAP_CNN1dBiLSTM.pt\"\n",
    "\n",
    "path = \"../data\" \n",
    "stim = \"Arousal\"\n",
    "\n",
    "par_list = list(range(32))\n",
    "num_segment = 60\n",
    "sss = StratifiedShuffleSplit(n_splits = 10, test_size = 0.25, random_state = 0)\n",
    "\n",
    "params = {\"batch_size\" : 16, \"shuffle\" : True, \"pin_memory\" : True}\n",
    "num_epochs = 50\n",
    "lr = 0.0001\n",
    "\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4160972-b510-4d29-9f6d-494b8222b8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#note that these params are simply obtained from trial and error; I got no theory to back up why I use certain numbers here...\n",
    "input_dim     = 32 #we got 32 EEG channels\n",
    "hidden_dim    = 256 #let's define hidden dim as 256\n",
    "num_layers    = 2  #we gonna have two LSTM layers\n",
    "output_dim    = 1  #we got 2 classes so we can output only 1 number, 0 for first class and 1 for another class\n",
    "bidirectional = True  #uses bidirectional LSTM\n",
    "dropout       = 0.5  #setting dropout to 0.5\n",
    "seq_len_first = False\n",
    "\n",
    "## LSTM is the only model that requires seq_len_first = True\n",
    "# seq_len_first = True\n",
    "class LSTM(nn.Module):\n",
    "    '''\n",
    "    Expected Input Shape: (batch, seq_len, channels)\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * num_layers, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [batch size, seq len, channels]\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        \n",
    "        #out = [batch size, seq len, hidden dim * num directions]        \n",
    "        #hn = [num layers * num directions, batch size, hidden dim]\n",
    "        #cn = [num layers * num directions, batch size, hidden dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        #hn = [batch size, hidden dim * num directions]\n",
    "        \n",
    "        return self.fc(hn)\n",
    "    \n",
    "class Conv1D_LSTM(nn.Module):\n",
    "    '''\n",
    "    Expected Input Shape: (batch, channels, seq_len)  <==what conv1d wants\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, bidirectional, dropout):\n",
    "        super(Conv1D_LSTM, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(input_dim, input_dim, kernel_size=16, stride=1, padding=1)\n",
    "        self.norm = nn.BatchNorm1d(input_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * num_layers, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # conv1d expects (batch, channels, seq_len)\n",
    "        # should not try too big a kernel size, which could lead to too much information loss\n",
    "        x = F.max_pool1d(F.relu(self.norm(self.conv1d(x))), kernel_size=3)\n",
    "        x = F.max_pool1d(F.relu(self.norm(self.conv1d(x))), kernel_size=3)\n",
    "\n",
    "        # change to (batch, seq_len, channels) because lstm expects\n",
    "        x = x.permute(0,2,1)\n",
    "        \n",
    "        #x = [batch size, seq len, channels]\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        \n",
    "        #out = [batch size, seq len, hidden dim * num directions]        \n",
    "        #hn = [num layers * num directions, batch size, hidden dim]\n",
    "        #cn = [num layers * num directions, batch size, hidden dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        #hn = [batch size, hidden dim * num directions]\n",
    "        \n",
    "        return self.fc(hn)\n",
    "\n",
    "######################################################################################################\n",
    "    \n",
    "class Conv1D_LSTM_Attention(nn.Module):\n",
    "    '''\n",
    "    Expected Input Shape: (batch, channels, seq_len)  <==what conv1d wants\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, bidirectional, dropout):\n",
    "        super(Conv1D_LSTM_Attention, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(input_dim, input_dim, kernel_size=16, stride=1, padding=1)\n",
    "        self.norm = nn.BatchNorm1d(input_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * num_layers, output_dim)\n",
    "    \n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        hidden = final_state.unsqueeze(2)  # hidden : [batch_size, n_hidden * num_directions(=2), 1(=n_layer)]\n",
    "        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2) # attn_weights : [batch_size, seq_len, 1]\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        # context = [batch_size, n_hidden * num_directions(=2), seq_len] * [batch_size, seq_len, 1] = [batch_size, n_hidden * num_directions(=2), 1]\n",
    "        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        return context, soft_attn_weights.cpu().data.numpy() # context : [batch_size, n_hidden * num_directions(=2)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # conv1d expects (batch, channels, seq_len)\n",
    "        # should not try too many conv1d or there is nothing to attend to, thus I have only left with one layer of conv1d\n",
    "        x = F.max_pool1d(F.relu(self.norm(self.conv1d(x))), kernel_size=3)\n",
    "\n",
    "        # change to (batch, seq_len, channels) because lstm expects\n",
    "        x = x.permute(0,2,1)\n",
    "        \n",
    "        #x = [batch size, seq len, channels]\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        \n",
    "        #out = [batch size, seq len, hidden dim * num directions]        \n",
    "        #hn = [num layers * num directions, batch size, hidden dim]\n",
    "        #cn = [num layers * num directions, batch size, hidden dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        #hn = [batch size, hidden dim * num directions]\n",
    "        \n",
    "        attn_output, attention = self.attention_net(out, hn)\n",
    "\n",
    "        return self.fc(attn_output)\n",
    "\n",
    "######################################################################################################\n",
    "len_reduction = 'mean' # 'sum' or 'last'\n",
    "\n",
    "class Conv1D_LSTM_SelfAttention(nn.Module):\n",
    "    '''\n",
    "    Expected Input Shape: (batch, channels, seq_len)  <==what conv1d wants\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, bidirectional, dropout, len_reduction='mean'):\n",
    "        super(Conv1D_LSTM_SelfAttention, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(input_dim, input_dim, kernel_size=16, stride=1, padding=1)\n",
    "        self.norm = nn.BatchNorm1d(input_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * num_layers, output_dim)\n",
    "        self.lin_Q = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.lin_K = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.lin_V = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.len_reduction = len_reduction\n",
    "    \n",
    "    # lstm_output : [batch_size, seq len, n_hidden * num_directions(=2)]\n",
    "    def self_attention_net(self, lstm_output):\n",
    "        q = self.lin_Q(torch.clone(lstm_output))\n",
    "        k = self.lin_K(torch.clone(lstm_output))\n",
    "        v = self.lin_V(torch.clone(lstm_output))\n",
    "        # q : [batch_size, seq_len, n_hidden * num_directions(=2)]\n",
    "        # k.transpose(1, 2): [batch_size, n_hidden * num_directions(=2), seq_len]\n",
    "        # attn_w = [batch_size, seq_len, seq_len]\n",
    "                \n",
    "        attn_w = torch.matmul(q, k.transpose(1, 2))\n",
    "        sfmx_attn_w = F.softmax(attn_w, 1)\n",
    "        \n",
    "        # context = [batch_size, seq_len, hidden_dim * num_directions(=2)]\n",
    "        context = torch.matmul(sfmx_attn_w, v)\n",
    "        \n",
    "        # by doing some mean/sum, the dimension on the seq len is gone\n",
    "        if self.len_reduction == \"mean\":\n",
    "            return torch.mean(context, dim=1), sfmx_attn_w.cpu().data.numpy()\n",
    "        elif self.len_reduction == \"sum\":\n",
    "            return torch.sum(context, dim=1), sfmx_attn_w.cpu().data.numpy()\n",
    "        elif self.len_reduction == \"last\":\n",
    "            return context[:, -1, :], sfmx_attn_w.cpu().data.numpy()   \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # conv1d expects (batch, channels, seq_len)\n",
    "        # should not try too big a kernel size, which could lead to too much information loss\n",
    "        x = F.max_pool1d(F.relu(self.norm(self.conv1d(x))), kernel_size=3)\n",
    "\n",
    "        # change to (batch, seq_len, channels) because lstm expects\n",
    "        x = x.permute(0,2,1)\n",
    "        \n",
    "        #x = [batch size, seq len, channels]\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        \n",
    "        #out = [batch size, seq len, hidden dim * num directions]        \n",
    "        #hn = [num layers * num directions, batch size, hidden dim]\n",
    "        #cn = [num layers * num directions, batch size, hidden dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        #hn = [batch size, hidden dim * num directions]\n",
    "        \n",
    "        attn_output, attention = self.self_attention_net(out)\n",
    "\n",
    "        return self.fc(attn_output)\n",
    "    \n",
    "###################################################################################################\n",
    "n_heads       = 8   #<=======new!\n",
    "d_k           = (hidden_dim * 2) // n_heads # (256 * 2) // 8\n",
    "len_reduction = 'mean'  # 'sum' or 'last'\n",
    "\n",
    "class Conv1D_LSTM_SelfMultiHeadAttention(nn.Module):\n",
    "    '''\n",
    "    Expected Input Shape: (batch, channels, seq_len)  <==what conv1d wants\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, bidirectional, dropout, len_reduction='mean'):\n",
    "        super(Conv1D_LSTM_SelfMultiHeadAttention, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(input_dim, input_dim, kernel_size=16, stride=1, padding=1)\n",
    "        self.norm = nn.BatchNorm1d(input_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.softmax       = nn.LogSoftmax(dim=1)\n",
    "        self.fc = nn.Linear(hidden_dim * num_layers, output_dim)\n",
    "        self.lin_Q = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.lin_K = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.lin_V = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.len_reduction = len_reduction\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
    "    \n",
    "    # lstm_output : [batch_size, seq len, n_hidden * num_directions(=2)]\n",
    "    def self_multihead_attention_net(self, lstm_output):\n",
    "        \n",
    "        residual, batch_size = lstm_output, lstm_output.size(0) #<---residual added to the last output; batch_size may not be even for the last unit\n",
    "        \n",
    "        q = self.lin_Q(torch.clone(lstm_output))\n",
    "        k = self.lin_K(torch.clone(lstm_output))\n",
    "        v = self.lin_V(torch.clone(lstm_output))\n",
    "        # q : [batch_size, seq_len, n_hidden * num_directions(=2)]\n",
    "        # k.transpose(1, 2): [batch_size, n_hidden * num_directions(=2), seq_len]\n",
    "        # attn_w = [batch_size, seq_len, seq_len]\n",
    "        \n",
    "        #split into heads\n",
    "        q = q.view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q: [batch_size x n_heads x seq_len x d_k]\n",
    "        k = k.view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k: [batch_size x n_heads x seq_len x d_k]\n",
    "        v = v.view(batch_size, -1, n_heads, d_k).transpose(1,2)  # v: [batch_size x n_heads x seq_len x d_k]\n",
    "        \n",
    "        # dot production attention\n",
    "        attn_w = torch.matmul(q, k.transpose(-1, -2)) / np.sqrt(d_k) # [batch_size x n_heads x seq_len x seq_len]\n",
    "                \n",
    "        sfmx_attn_w = self.softmax(attn_w)\n",
    "        context = torch.matmul(sfmx_attn_w, v) # [batch_size x n_heads x seq_len x d_k]\n",
    "        \n",
    "        # concatenate heads\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_k) # context: [batch_size x seq_len x n_heads * d_k]\n",
    "        # now context: [batch_size, seq_len, hidden_dim * 2]\n",
    "        \n",
    "        # doing skip connection\n",
    "        # https://stats.stackexchange.com/questions/474440/why-do-transformers-use-layer-norm-instead-of-batch-norm\n",
    "        context = self.layer_norm(residual + context)\n",
    "\n",
    "        if self.len_reduction == \"mean\":\n",
    "            return torch.mean(context, dim=1), sfmx_attn_w.cpu().data.numpy()\n",
    "        elif self.len_reduction == \"sum\":\n",
    "            return torch.sum(context, dim=1), sfmx_attn_w.cpu().data.numpy()\n",
    "        elif self.len_reduction == \"last\":\n",
    "            return context[:, -1, :], sfmx_attn_w.cpu().data.numpy()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # conv1d expects (batch, channels, seq_len)\n",
    "        # should not try too big a kernel size, which could lead to too much information loss\n",
    "        x = F.max_pool1d(F.relu(self.norm(self.conv1d(x))), kernel_size=3)\n",
    "\n",
    "        # change to (batch, seq_len, channels) because lstm expects\n",
    "        x = x.permute(0,2,1)\n",
    "        \n",
    "        #x = [batch size, seq len, channels]\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        \n",
    "        #out = [batch size, seq len, hidden dim * num directions]        \n",
    "        #hn = [num layers * num directions, batch size, hidden dim]\n",
    "        #cn = [num layers * num directions, batch size, hidden dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        #hn = [batch size, hidden dim * num directions]\n",
    "        \n",
    "        attn_output, attention = self.self_multihead_attention_net(out)\n",
    "\n",
    "        return self.fc(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7713d60e-1116-4d55-855d-ffb8fdbdf3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model Conv1D_LSTM_SelfMultiHeadAttention has 2,976,865 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = Conv1D_LSTM_SelfMultiHeadAttention(input_dim, hidden_dim, num_layers, output_dim, bidirectional, dropout, len_reduction)\n",
    "model = model.to(device)  \n",
    "model.apply(initialize_weights)\n",
    "print(f'The model {type(model).__name__} has {count_parameters(model):,} trainable parameters')# Train the model\n",
    "\n",
    "def reset_model():\n",
    "    model = Conv1D_LSTM_SelfAttention(input_dim, hidden_dim, num_layers, output_dim, bidirectional, dropout, len_reduction)\n",
    "    model = model.to(device)  \n",
    "    model.apply(initialize_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    return model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ec93d13-7de0-4937-864c-fce96bcccdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Par :  s01.dat\n",
      "Training Par : 0 | Fold 0\n",
      "Epoch: 01 | Epoch Time: 0m 2s\n",
      "\t Train Loss: 0.678 | Train Acc: 59.96%\n",
      "\t Val. Loss: 0.675  |  Val. Acc: 59.70%\n",
      "Epoch: 02 | Epoch Time: 0m 2s\n",
      "\t Train Loss: 0.669 | Train Acc: 60.62%\n",
      "\t Val. Loss: 0.672  |  Val. Acc: 60.36%\n",
      "Epoch: 03 | Epoch Time: 0m 2s\n",
      "\t Train Loss: 0.591 | Train Acc: 68.53%\n",
      "\t Val. Loss: 0.721  |  Val. Acc: 62.99%\n",
      "Epoch: 04 | Epoch Time: 0m 2s\n",
      "\t Train Loss: 0.501 | Train Acc: 75.22%\n",
      "\t Val. Loss: 0.715  |  Val. Acc: 54.44%\n",
      "Epoch: 05 | Epoch Time: 0m 2s\n",
      "\t Train Loss: 0.468 | Train Acc: 77.49%\n",
      "\t Val. Loss: 0.734  |  Val. Acc: 64.47%\n",
      "Epoch: 06 | Epoch Time: 0m 2s\n",
      "\t Train Loss: 0.445 | Train Acc: 78.54%\n",
      "\t Val. Loss: 0.765  |  Val. Acc: 67.11%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29376/1124200947.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         train_loss, train_acc, valid_loss, valid_acc = train(num_epochs,\n\u001b[0m\u001b[1;32m     47\u001b[0m                                                              \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                                                              \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_work/EEG-Emotion-Recognition/projects/components/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, model, train_loader, val_loader, optimizer, criterion, model_name, device, seq_len_first)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_work/EEG-Emotion-Recognition/projects/components/train.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(model, train_loader, optimizer, criterion, device, seq_len_first)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m#backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/archive/gpu/home/users/jakrapop.a/.conda/envs/jakrapop_env39/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/archive/gpu/home/users/jakrapop.a/.conda/envs/jakrapop_env39/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result_dict = {}\n",
    "\n",
    "for par in par_list:\n",
    "\n",
    "    if debug and par == 1:\n",
    "        break\n",
    "    \n",
    "    result_dict[par] = {}\n",
    "    \n",
    "    # ==== GET ALL DATA OF THIS PAR ====\n",
    "    all_data, all_label = get_par_data(path, par, stim)\n",
    "\n",
    "    # ==== GET CV DATA OF THAT PARTICIPANT FOR EACH FOLD ====\n",
    "    for i_fold, (train_index, test_index) in enumerate(sss.split(all_data, all_label)):\n",
    "        \n",
    "        if debug and i_fold == 1:\n",
    "            break\n",
    "        \n",
    "        result_dict[par][i_fold] = {}\n",
    "        \n",
    "        print(f\"Training Par : {par} | Fold {i_fold}\")\n",
    "\n",
    "        X_train, X_test = all_data[train_index]  , all_data[test_index]\n",
    "        y_train, y_test = all_label[train_index] , all_label[test_index]\n",
    "\n",
    "        # === PERFORM SEGMENTATION on TRAIN and TEST set === \n",
    "\n",
    "        train_data, train_label = get_segmented_data(X_train, y_train, num_segment)\n",
    "        test_data,  test_label  = get_segmented_data(X_test,  y_test, num_segment)\n",
    "        del  X_train, X_test, y_train, y_test\n",
    "\n",
    "        train_dataset = TensorDataset(torch.tensor(train_data).float(), torch.tensor(train_label).float())\n",
    "        test_dataset  = TensorDataset(torch.tensor(test_data).float(), torch.tensor(test_label).float())\n",
    "        del train_data, train_label, test_data, test_label\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, **params)\n",
    "        val_loader  = DataLoader(test_dataset, **params)\n",
    "\n",
    "        # === RESET MODEL ===\n",
    "        model, optimizer, criterion = reset_model()\n",
    "        \n",
    "        # === DO TRAINING === \n",
    "        train_loss, train_acc, valid_loss, valid_acc = train(num_epochs,\n",
    "                                                             model,\n",
    "                                                             train_loader,\n",
    "                                                             val_loader,\n",
    "                                                             optimizer,\n",
    "                                                             criterion, model_saved_name,\n",
    "                                                             device,\n",
    "                                                             seq_len_first)\n",
    "        result_dict[par][i_fold]['train_loss'] = train_loss\n",
    "        result_dict[par][i_fold]['train_acc']  = train_acc\n",
    "        result_dict[par][i_fold]['valid_loss'] = valid_loss\n",
    "        result_dict[par][i_fold]['valid_acc']  = valid_acc\n",
    "        \n",
    "        del model, optimizer, criterion, train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9002a9-aa07-41f9-bc6a-4ce02c0fe61a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b6dcbf-ced8-47fb-bfc7-aa647cf22c90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
