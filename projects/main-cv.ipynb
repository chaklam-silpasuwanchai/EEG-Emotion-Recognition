{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3eab870-2294-405a-b31d-9bfccb5ae4db",
   "metadata": {},
   "source": [
    "# Main 10-Fold Cross-Validation Subject Dependent Split first then Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb04369f-de64-426d-8188-c125d9aa4dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from components.models import *\n",
    "from components.helper import *\n",
    "from components.dataset_jo import *\n",
    "from components.train import *\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb83d369-eb48-4594-b692-c3080f30897d",
   "metadata": {},
   "source": [
    "## Training Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da7aef09-9a9d-4a19-ad62-58ed77881a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Config():\n",
    "    def __init__(self):\n",
    "        \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        LSTM\n",
    "        Conv1D_LSTM\n",
    "        Conv1D_LSTM_Attention\n",
    "        Conv1D_LSTM_SelfAttention\n",
    "        Conv1D_LSTM_MultiHeadSelfAttention\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        # set running mode : juypyter or py\n",
    "        # - jupyter = testing mode\n",
    "        # - py      = production mode\n",
    "        parser  = argparse.ArgumentParser()\n",
    "        parser.add_argument('-a', '--model_name',    help='model_name' , type=str, required=False)\n",
    "        parser.add_argument('-x', '--stim',          help='stim' ,       type=int, required=False)\n",
    "        parser.add_argument('-s', '--segment',       help='segment' ,    type=int, required=False)\n",
    "        parser.add_argument('-l', '--len_reduction', help='len_reduction' , type=str, required=False)\n",
    "        parser.add_argument('-f', '--isdebug',       help='Set running mode' , type=str, required=False)\n",
    "        args     = parser.parse_args()\n",
    "\n",
    "        if args.isdebug == 'yes' or 'json' in args.isdebug :\n",
    "            print(\"Jupyter mode\")\n",
    "            model_name    = 'LSTM'\n",
    "            stim          = 1\n",
    "            len_reduction = 'mean'  # 'mean'  or 'sum' or 'last'\n",
    "            segment       = 1 # 1, 3, 5\n",
    "\n",
    "        else:\n",
    "            model_name    = str(args.model_name)\n",
    "            stim          = int(args.stim)\n",
    "            segment       = int(args.segment)\n",
    "            len_reduction = str(args.len_reduction)  # 'none' or 'mean' or 'sum' or 'last'\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "               \n",
    "        \n",
    "        ##============================================\n",
    "        #  !!!!!!!!!!!!     DO NOT EDIT BELOW\n",
    "        #============================================\n",
    "        \n",
    "        \n",
    "        self.device = 'cpu'\n",
    "\n",
    "        #========== Training Configurations==========\n",
    "        self.path = \"../data\" \n",
    "        \n",
    "        \n",
    "        # STIMULI_VALENCE = 0\n",
    "        # STIMULI_AROUSAL = 1       \n",
    "        self.stim      = stim\n",
    "        self.stim_name = 'VALENCE' if self.stim else 'AROUSAL'\n",
    "        self.segment   = segment\n",
    "\n",
    "        self.params     = {\"batch_size\" : 16, \"shuffle\" : True, \"pin_memory\" : True}\n",
    "        self.num_epochs = 50\n",
    "        self.lr         = 0.0001\n",
    "\n",
    "        # true only if using 'LSTM'\n",
    "        if model_name == 'LSTM' :\n",
    "            self.seq_len_first = True\n",
    "        else :\n",
    "            self.seq_len_first = False\n",
    "\n",
    "        self.debug = False\n",
    "        if self.debug:\n",
    "            self.num_epochs = 1\n",
    "            self.n_split    = 3\n",
    "\n",
    "        #========== Model Configurations==========\n",
    "        # model list \n",
    "\n",
    "        \n",
    "        \n",
    "        self.model_name    = model_name   # this should be match with the model class\n",
    "        self.input_dim     = 32   # we got 32 EEG channels\n",
    "        self.hidden_dim    = 256  # let's define hidden dim as 256\n",
    "        self.num_layers    = 2    # we gonna have two LSTM layers\n",
    "        self.output_dim    = 1    # we got 2 classes so we can output only 1 number, 0 for first class and 1 for another class\n",
    "        self.bidirectional = True # uses bidirectional LSTM\n",
    "        self.dropout       = 0.5  # setting dropout to 0.5\n",
    "\n",
    "        # for self attention\n",
    "        self.len_reduction = len_reduction\n",
    "\n",
    "        # for multi head attention\n",
    "        self.n_heads       = 8\n",
    "        self.d_k           = (self.hidden_dim * 2) // self.n_heads # (256 * 2) // 8\n",
    "        \n",
    "        \n",
    "        #========== save config ==========\n",
    "        self.segsplit      = 'split'\n",
    "        self.output_path   = f'./output/{self.segsplit}_{int(60/self.segment)}s/'\n",
    "        self.result_csv    = f'{self.output_path}{self.model_name}_result.csv'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef506e0a-ea2f-4e6b-8730-71856a931eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter mode\n",
      "device : cpu\n",
      "path : ../data\n",
      "stim : 1\n",
      "stim_name : VALENCE\n",
      "segment : 1\n",
      "params : {'batch_size': 16, 'shuffle': True, 'pin_memory': True}\n",
      "num_epochs : 50\n",
      "lr : 0.0001\n",
      "seq_len_first : True\n",
      "debug : False\n",
      "model_name : LSTM\n",
      "input_dim : 32\n",
      "hidden_dim : 256\n",
      "num_layers : 2\n",
      "output_dim : 1\n",
      "bidirectional : True\n",
      "dropout : 0.5\n",
      "len_reduction : mean\n",
      "n_heads : 8\n",
      "d_k : 64\n",
      "segsplit : split\n",
      "output_path : ./output/split_60s/\n",
      "result_csv : ./output/split_60s/LSTM_result.csv\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "print_cls_var( config )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c2fb70-7d34-4f9d-8e99-2c6a755c2624",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c582275-b1ec-4d0f-9fde-bc9117ce3aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model( config ):\n",
    "    \n",
    "    if config.model_name == 'LSTM' :\n",
    "        model = LSTM( config.input_dim, \n",
    "                     config.hidden_dim, \n",
    "                     config.num_layers, \n",
    "                     config.output_dim, \n",
    "                     config.bidirectional, \n",
    "                     config.dropout)\n",
    "        \n",
    "    elif config.model_name == 'Conv1D_LSTM' :\n",
    "        model = Conv1D_LSTM( config.input_dim, \n",
    "                            config.hidden_dim, \n",
    "                            config.num_layers, \n",
    "                            config.output_dim, \n",
    "                            config.bidirectional, \n",
    "                            config.dropout\n",
    "                           )\n",
    "    elif config.model_name == 'Conv1D_LSTM_Attention' :\n",
    "        model = Conv1D_LSTM_Attention ( config.input_dim, \n",
    "                                       config.hidden_dim, \n",
    "                                       config.num_layers, \n",
    "                                       config.output_dim, \n",
    "                                       config.bidirectional, \n",
    "                                       config.dropout\n",
    "                                      )\n",
    "\n",
    "    elif config.model_name == 'Conv1D_LSTM_SelfAttention' :\n",
    "        model = Conv1D_LSTM_SelfAttention( config.input_dim, \n",
    "                                  config.hidden_dim, \n",
    "                                  config.num_layers, \n",
    "                                  config.output_dim, \n",
    "                                  config.bidirectional, \n",
    "                                  config.dropout, \n",
    "                                  config.len_reduction   \n",
    "                                 )\n",
    "    elif config.model_name == 'Conv1D_LSTM_MultiHeadSelfAttention' :\n",
    "        model =Conv1D_LSTM_MultiHeadSelfAttention( config.input_dim, \n",
    "                                                  config.hidden_dim, \n",
    "                                                  config.num_layers, \n",
    "                                                  config.output_dim, \n",
    "                                                  config.bidirectional, \n",
    "                                                  config.dropout, \n",
    "                                                  config.len_reduction,\n",
    "                                                  config.n_heads,\n",
    "                                                  config.d_k\n",
    "                                                 )\n",
    "    \n",
    "    \n",
    "    model = model.to(config.device)  \n",
    "    model.apply(initialize_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr) \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    \n",
    "    return model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b90c946-f9b4-44ff-940d-6d8e0e317a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model LSTM has 2,171,393 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model, _, _ = init_model( config )\n",
    "print(f'The model {type(model).__name__} has {count_parameters(model):,} trainable parameters')# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b98ed7c-c5e2-41a4-babe-464acec8aaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 32 files\n",
      "['s01', 's02', 's03', 's04', 's05', 's06', 's07', 's08', 's09', 's10', 's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21', 's22', 's23', 's24', 's25', 's26', 's27', 's28', 's29', 's30', 's31', 's32']\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset_subjectDependent(config.path)\n",
    "dataset.set_segment(config.segment)\n",
    "\n",
    "filenames = dataset.get_file_list()\n",
    "filenames.sort()\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "666a316d-8455-41f6-b438-bff439fec426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reset_model():\n",
    "#     model = LSTM(input_dim, hidden_dim, num_layers, output_dim, bidirectional, dropout)\n",
    "#     model = model.to(device)  \n",
    "#     model.apply(initialize_weights)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr) \n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "#     return model, optimizer, criterion\n",
    "\n",
    "def make_dataloader(X_orig, y_orig, train_idxs, test_idxs, params):\n",
    "    \n",
    "    X_train, X_test = X_orig[train_idxs] , X_orig[test_idxs]\n",
    "    y_train, y_test = y_orig[train_idxs] , y_orig[test_idxs]\n",
    "\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train).float(), torch.tensor(y_train).float())\n",
    "    test_dataset  = TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_test).float())\n",
    "    del X_train, X_test, y_train, y_test\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, **params)\n",
    "    val_loader   = DataLoader(test_dataset, **params)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6f7c701-0cbe-41d0-8043-ce32ef68ac47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Participant :  s01\n",
      "s01 (40, 32, 7680) (40,) (40,)\n",
      "---------------------\n",
      "fold :  0\n",
      "(30,) (10,) set()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-63981ebc3de6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# === DO TRAINING ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         train_loss, valid_loss, train_acc , valid_acc , epoch_times = train(config.num_epochs,\n\u001b[0m\u001b[1;32m     32\u001b[0m                                                              \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                                                              \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_work/EEG-Emotion-Recognition/projects/components/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, model, train_loader, val_loader, optimizer, criterion, device, seq_len_first)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_work/EEG-Emotion-Recognition/projects/components/train.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(model, train_loader, optimizer, criterion, device, seq_len_first)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m#predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#output shape: (batch, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/archive/gpu/home/users/jakrapop.a/.conda/envs/jakrapop_nlu/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_work/EEG-Emotion-Recognition/projects/components/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#x = [batch size, seq len, channels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#out = [batch size, seq len, hidden dim * num directions]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/archive/gpu/home/users/jakrapop.a/.conda/envs/jakrapop_nlu/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/archive/gpu/home/users/jakrapop.a/.conda/envs/jakrapop_nlu/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    692\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result_dict = {}\n",
    "\n",
    "for filename in filenames:\n",
    "    print(\"==================================================\")\n",
    "    print(\"Participant : \", filename )\n",
    "    result_dict[filename] = {}\n",
    "    \n",
    "    \n",
    "    # get participant dataset\n",
    "    X, y, groups = dataset.get_data(filename, Dataset_subjectDependent.STIMULI_AROUSAL, return_type='numpy')\n",
    "    print(filename, X.shape, y.squeeze().shape, groups.shape)\n",
    "\n",
    "    cv = GroupShuffleSplit(n_splits=10, train_size=0.75, random_state=0)\n",
    "    \n",
    "    # get each fold for training and testing model\n",
    "\n",
    "    for fold, ( train_idxs, test_idxs ) in enumerate( cv.split(X, y.squeeze(), groups)):\n",
    "\n",
    "        print(\"---------------------\")\n",
    "        print( \"fold : \", fold )\n",
    "        print(train_idxs.shape, test_idxs.shape, set(groups[train_idxs]).intersection(groups[test_idxs]) )\n",
    "        \n",
    "\n",
    "        X_orig, y_orig = X.copy(), y.copy()\n",
    "        train_loader, val_loader = make_dataloader(X_orig, y_orig, train_idxs, test_idxs, config.params)\n",
    "        \n",
    "        # === Init MODEL ===\n",
    "        model, optimizer, criterion = init_model( config )\n",
    "        \n",
    "        # === DO TRAINING === \n",
    "        train_loss, valid_loss, train_acc , valid_acc , epoch_times = train(config.num_epochs,\n",
    "                                                             model,\n",
    "                                                             train_loader,\n",
    "                                                             val_loader,\n",
    "                                                             optimizer,\n",
    "                                                             criterion,\n",
    "                                                             config.device,\n",
    "                                                              config.seq_len_first)\n",
    "        \n",
    "        del model, optimizer, criterion, train_loader, val_loader\n",
    "\n",
    "        # save to csv at specific epoch\n",
    "        for epoch in range( config.num_epochs ) :\n",
    "                result_csv_dic               = {}\n",
    "                result_csv_dic['len_reduction']  =  config.len_reduction\n",
    "                result_csv_dic['par']        =  filename\n",
    "                result_csv_dic['stim_name']  =  config.stim_name\n",
    "                result_csv_dic['fold']       =  fold\n",
    "                result_csv_dic['epoch']      =  epoch\n",
    "                result_csv_dic['train_loss'] = train_loss[epoch]\n",
    "                result_csv_dic['valid_loss'] = valid_loss[epoch]\n",
    "                result_csv_dic['train_acc']  = train_acc[epoch]\n",
    "                result_csv_dic['valid_acc']  = valid_acc[epoch]\n",
    "                result_csv_dic['epoch_time'] = epoch_times[epoch]\n",
    "                save_result_csv( result_csv_dic, config.result_csv )\n",
    "                \n",
    "            \n",
    "        # ## save dictionary of all output result\n",
    "        # result_dict[filename][fold]['train_loss'].append(train_loss)\n",
    "        # result_dict[filename][fold]['train_acc'].append(train_acc)\n",
    "        # result_dict[filename][fold]['valid_loss'].append(valid_loss)\n",
    "        # result_dict[filename][fold]['valid_acc'].append(valid_acc)\n",
    "        # result_dict[filename][fold]['epoch_mins'].append(epoch_mins)\n",
    "        # result_dict[filename][fold]['epoch_secs'].append(epoch_secs)      \n",
    "        # with open(f'{config.output_path}{config.model_name}_{config.stim_name}_output_dic', 'wb') as outp:\n",
    "        #     pickle.dump(result_dict, outp, pickle.HIGHEST_PROTOCOL)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccdee39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb4023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jakrapop_nlu",
   "language": "python",
   "name": "jakrapop_nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
