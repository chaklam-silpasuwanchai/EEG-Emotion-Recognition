{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ea1da9-b84e-4fb2-80b1-5667f305af91",
   "metadata": {},
   "source": [
    "# How to use Get dataset functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4487823f-91fd-4cd8-a87b-2d5576809c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# setting seed so that splitting process and training process can be reproduce\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#refactoring components\n",
    "#maybe you want to take a look at these components, because we will be using them in the future series\n",
    "from components.dataset import Dataset\n",
    "from components.helper import getLoaders, count_parameters, plot_performance\n",
    "from components.train import train, evaluate, initialize_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5c1058-3f11-4135-8366-3b9e701c5364",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927c024e-2ffd-453e-8aca-d9a8502504d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available Functions\n",
    "# all function and class are already in dataset.py\n",
    "from components.dataset import *\n",
    "\n",
    "path = \"data\" \n",
    "stim = \"Arousal\"\n",
    "par  = 5\n",
    "\n",
    "# Subject Dependent\n",
    "train_set, val_set, test_set = get_SplitSegSubjectDependentDatasets(path, stim, par)\n",
    "train_set, val_set, test_set = get_SegSplitSubjectDependentDatasets(path, stim, par)\n",
    "\n",
    "# Subject Independent\n",
    "train_set, val_set, test_set = get_TransferLearningDatasets(path, stim)\n",
    "train_set, val_set, test_set = get_SplitSegPoolDataset(path, stim)\n",
    "train_set, val_set, test_set = get_SegSplitPoolDatasets(path, stim)\n",
    "\n",
    "# Non-Segmented Dataset (can truncate to specific length)\n",
    "train_set, val_set, test_set = get_NonSegmentOneParDatasets(path, stim, par, truncate = False)\n",
    "train_set, val_set, test_set = get_NonSegmentAllParDatasets(path, stim, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a048c4c8-467c-4fd7-b41d-9d4e4f7ea0ec",
   "metadata": {},
   "source": [
    "## 1 ) Subject Dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebb4019-c8e6-48f4-9f08-7c48f1c3a213",
   "metadata": {},
   "source": [
    "### 1.1) Subject Dependent : Split video then do Segmentation\n",
    "\n",
    "- Use data from only one participant for Train Val and Test\n",
    "- Segments from one video will only be in one Split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3e8d299-64a5-434a-8bbd-9c3c5d75ba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, all_data, all_label, indices):\n",
    "        self.data   = all_data[indices]\n",
    "        self.label  = all_label[indices]\n",
    "        shape = self.data.shape\n",
    "        \n",
    "        #perform segmentation=====\n",
    "        segments = 12\n",
    "        \n",
    "        self.data = self.data.reshape(shape[0], shape[1], int(shape[2]/segments), segments)\n",
    "        #train data shape: (896, 32, 672, 12)\n",
    "\n",
    "        self.data = self.data.transpose(0, 3, 1, 2)\n",
    "        #train data shape: (896, 12, 32, 672)\n",
    "\n",
    "        self.data = self.data.reshape(shape[0] * segments, shape[1], -1)\n",
    "        #train data shape: (896*12, 32, 672)\n",
    "        #==========================\n",
    "        self.label = np.repeat(self.label, segments)[:, np.newaxis]  #the dimension 1 is lost after repeat, so need to unsqueeze (896*12, 1)\n",
    "        del all_data, all_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        single_data  = self.data[idx]\n",
    "        single_label = (self.label[idx] > 5).astype(float)   #convert the scale to either 0 or 1 (to classification problem)\n",
    "        batch = {\n",
    "            'data' : torch.Tensor(single_data),\n",
    "            'label': torch.Tensor(single_label)\n",
    "        }\n",
    "        return batch\n",
    "    \n",
    "def get_SplitSegSubjectDependentDatasets(path, stim, par):\n",
    "    \n",
    "    # ==== GET FILENAME OF THE PARTICIPANT THAT WE WANT ====\n",
    "    _, _, filenames = next(os.walk(path))\n",
    "    filenames = sorted(filenames)\n",
    "    par_filename = filenames[par]\n",
    "    print(par_filename)\n",
    "    \n",
    "    # ==== GET ALL DATA OF THAT PARTICIPANT ====\n",
    "    all_data  = []\n",
    "    all_label = []\n",
    "    temp = pickle.load(open(os.path.join(path, par_filename), 'rb'), encoding='latin1')\n",
    "    all_data.append(temp['data'])\n",
    "    if stim == \"Valence\":\n",
    "        all_label.append(temp['labels'][:,:1])  # first index is valence\n",
    "    elif stim == \"Arousal\":\n",
    "        all_label.append(temp['labels'][:,1:2]) # second index is arousal\n",
    "    all_data  = np.vstack(all_data)[:, :32, ]   # shape: (1280, 32, 8064) --> take only the first 32 channels\n",
    "    all_label = np.vstack(all_label)           # (1280, 1)  ==> 1280 samples, \n",
    "    \n",
    "    # ==== SPLIT DATA OF THAT PARTICIPANT ====\n",
    "    len_all_ds = len(all_data)\n",
    "    torch.manual_seed(1)\n",
    "    indices   = torch.randperm(len_all_ds).tolist()\n",
    "    train_ind = int(0.85 * len_all_ds)\n",
    "    val_ind   = int(0.95 * len_all_ds)\n",
    "    train_idx = indices[ : train_ind] \n",
    "    val_idx   = indices[train_ind : val_ind] \n",
    "    test_idx  = indices[val_ind :]\n",
    "\n",
    "    # ==== GET SEGMENTED DATASET OF THAT PARTICIPANT ====\n",
    "    train_set = SegDataset(all_data, all_label, train_idx) \n",
    "    val_set   = SegDataset(all_data, all_label, val_idx)\n",
    "    test_set  = SegDataset(all_data, all_label, test_idx)\n",
    "    del all_data, all_label\n",
    "    \n",
    "    data_train  = train_set[:]['data']\n",
    "    label_train = train_set[:]['label']\n",
    "    data_val    = val_set[:]['data']\n",
    "    label_val   = val_set[:]['label']\n",
    "    data_test   = test_set[:]['data']\n",
    "    label_test  = test_set[:]['label']\n",
    "\n",
    "    print(\"Train Data shape : \" , data_train.shape)\n",
    "    print(\"Train Label shape: \" , label_train.shape) \n",
    "    print(\"val Data shape   : \" , data_val.shape) \n",
    "    print(\"val Label shape  : \" , label_val.shape) \n",
    "    print(\"test Data shape  : \" , data_test.shape)  \n",
    "    print(\"test Label shape : \" , label_test.shape)    \n",
    "    \n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baa19486-28d7-48ef-9ffa-ac98901d6566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s06.dat\n",
      "Train Data shape :  torch.Size([408, 32, 672])\n",
      "Train Label shape:  torch.Size([408, 1])\n",
      "val Data shape   :  torch.Size([48, 32, 672])\n",
      "val Label shape  :  torch.Size([48, 1])\n",
      "test Data shape  :  torch.Size([24, 32, 672])\n",
      "test Label shape :  torch.Size([24, 1])\n"
     ]
    }
   ],
   "source": [
    "path = path = \"data\" \n",
    "stim = \"Arousal\"\n",
    "par  = 5\n",
    "\n",
    "train_set, val_set, test_set = get_SplitSegSubjectDependentDatasets(path, stim, par)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f942cf3-ec52-414b-beb5-a1211d1056e8",
   "metadata": {},
   "source": [
    "### 1.2) Subject Dependent : Segment data then do Split\n",
    "\n",
    "- Use data from only one participant for Train Val and Test\n",
    "- Segments from one video can be in Train/Val/Test Split at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff9c4298-703d-470e-aae5-fe8ee9c200c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, all_data, all_label, indices):\n",
    "        self.data   = all_data[indices]\n",
    "        self.label  = all_label[indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        single_data  = self.data[idx]\n",
    "        single_label = (self.label[idx] > 5).astype(float)   #convert the scale to either 0 or 1 (to classification problem)\n",
    "        batch = {\n",
    "            'data' : torch.Tensor(single_data),\n",
    "            'label': torch.Tensor(single_label)\n",
    "        }\n",
    "        return batch\n",
    "    \n",
    "def get_SegSplitSubjectDependentDatasets(path, stim, par):\n",
    "    \n",
    "    # ==== GET FILENAME OF THE PARTICIPANT THAT WE WANT ====\n",
    "    _, _, filenames = next(os.walk(path))\n",
    "    filenames = sorted(filenames)\n",
    "    par_filename = filenames[par]\n",
    "    print(par_filename)\n",
    "    \n",
    "    # ==== GET ALL DATA OF THAT PARTICIPANT ====\n",
    "    all_data  = []\n",
    "    all_label = []\n",
    "    temp = pickle.load(open(os.path.join(path, par_filename), 'rb'), encoding='latin1')\n",
    "    all_data.append(temp['data'])\n",
    "    if stim == \"Valence\":\n",
    "        all_label.append(temp['labels'][:,:1])  # first index is valence\n",
    "    elif stim == \"Arousal\":\n",
    "        all_label.append(temp['labels'][:,1:2]) # second index is arousal\n",
    "    all_data  = np.vstack(all_data)[:, :32, ]   # shape: (1280, 32, 8064) --> take only the first 32 channels\n",
    "    all_label = np.vstack(all_label)           # (1280, 1)  ==> 1280 samples, \n",
    "         \n",
    "    # ==== DO SEGMENTATION ====\n",
    "    shape = all_data.shape\n",
    "    segments = 12\n",
    "    all_data = all_data.reshape(shape[0], shape[1], int(shape[2]/segments), segments)\n",
    "    #train data shape: (896, 32, 672, 12)\n",
    "    all_data = all_data.transpose(0, 3, 1, 2)\n",
    "    #train data shape: (896, 12, 32, 672)\n",
    "    all_data = all_data.reshape(shape[0] * segments, shape[1], -1)\n",
    "    #train data shape: (896*12, 32, 672)\n",
    "    all_label = np.repeat(all_label, segments)[:, np.newaxis]  #the dimension 1 is lost after repeat, so need to unsqueeze (896*12, 1)\n",
    "    \n",
    "    # ==== SPLIT SEGMENTED DATA ====\n",
    "    len_all_ds = len(all_data)\n",
    "    torch.manual_seed(1)\n",
    "    indices   = torch.randperm(len_all_ds).tolist()\n",
    "    train_ind = int(0.85 * len_all_ds)\n",
    "    val_ind   = int(0.95 * len_all_ds)\n",
    "    train_idx = indices[ : train_ind] \n",
    "    val_idx   = indices[train_ind : val_ind] \n",
    "    test_idx  = indices[val_ind :]\n",
    "    \n",
    "    # ==== GET SPLIT DATASET ====\n",
    "    train_set = SplitDataset(all_data, all_label, train_idx) \n",
    "    val_set   = SplitDataset(all_data, all_label, val_idx)\n",
    "    test_set  = SplitDataset(all_data, all_label, test_idx)\n",
    "    del all_data, all_label\n",
    "    \n",
    "    data_train  = train_set[:]['data']\n",
    "    label_train = train_set[:]['label']\n",
    "    data_val    = val_set[:]['data']\n",
    "    label_val   = val_set[:]['label']\n",
    "    data_test   = test_set[:]['data']\n",
    "    label_test  = test_set[:]['label']\n",
    "\n",
    "    print(\"Train Data shape : \" , data_train.shape)\n",
    "    print(\"Train Label shape: \" , label_train.shape) \n",
    "    print(\"val Data shape   : \" , data_val.shape) \n",
    "    print(\"val Label shape  : \" , label_val.shape) \n",
    "    print(\"test Data shape  : \" , data_test.shape)  \n",
    "    print(\"test Label shape : \" , label_test.shape) \n",
    "    \n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac57b10e-5915-40ac-9513-92ee8109d6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s06.dat\n",
      "Train Data shape :  torch.Size([408, 32, 672])\n",
      "Train Label shape:  torch.Size([408, 1])\n",
      "val Data shape   :  torch.Size([48, 32, 672])\n",
      "val Label shape  :  torch.Size([48, 1])\n",
      "test Data shape  :  torch.Size([24, 32, 672])\n",
      "test Label shape :  torch.Size([24, 1])\n"
     ]
    }
   ],
   "source": [
    "path = \"data\" \n",
    "stim = \"Arousal\"\n",
    "par  = 5\n",
    "\n",
    "train_set, val_set, test_set = get_SegSplitSubjectDependentDatasets(path, stim, par)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab3bad5-bcb9-43c1-81d8-261784f1ac2d",
   "metadata": {},
   "source": [
    "## 2) Subject Independent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d446cd4-e944-40c6-86e6-d25ef82263bc",
   "metadata": {},
   "source": [
    "### 2.1 ) Transfer Learning (Leave One Out)\n",
    "\n",
    "- Split at participant level\n",
    "- Participant in Train Set will not be in Validation or Test set\n",
    "(Similar to Leave One Out but we leave more than one participant out as test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f39e9d7e-9e7c-4908-bbf4-a7ba8c13ae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferLearningDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path, stim, par_filename):\n",
    "        all_data = []\n",
    "        all_label = []\n",
    "        for dat in par_filename:\n",
    "            temp = pickle.load(open(os.path.join(path,dat), 'rb'), encoding='latin1')\n",
    "            all_data.append(temp['data'])\n",
    "            if stim == \"Valence\":\n",
    "                all_label.append(temp['labels'][:,:1])   #the first index is valence\n",
    "            elif stim == \"Arousal\":\n",
    "                all_label.append(temp['labels'][:,1:2]) # Arousal  #the second index is arousal\n",
    "                \n",
    "        self.data = np.vstack(all_data)[:, :32, ]   #shape: (1280, 32, 8064) --> take only the first 32 channels\n",
    "        self.label = np.vstack(all_label) #(1280, 1)  ==> 1280 samples, \n",
    "        \n",
    "        shape = self.data.shape\n",
    "        \n",
    "        # ==== DO SEGMENTATION ====\n",
    "        segments = 12\n",
    "        \n",
    "        self.data = self.data.reshape(shape[0], shape[1], int(shape[2]/segments), segments)\n",
    "        #train data shape: (896, 32, 672, 12)\n",
    "\n",
    "        self.data = self.data.transpose(0, 3, 1, 2)\n",
    "        #train data shape: (896, 12, 32, 672)\n",
    "\n",
    "        self.data = self.data.reshape(shape[0] * segments, shape[1], -1)\n",
    "        #train data shape: (896*12, 32, 672)\n",
    "        #==========================\n",
    "        \n",
    "        self.label = np.repeat(self.label, segments)[:, np.newaxis]  #the dimension 1 is lost after repeat, so need to unsqueeze (896*12, 1)\n",
    "        \n",
    "        del all_data, all_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        single_data  = self.data[idx]\n",
    "        single_label = (self.label[idx] > 5).astype(float)   #convert the scale to either 0 or 1 (to classification problem)\n",
    "        \n",
    "        batch = {\n",
    "            'data' : torch.Tensor(single_data),\n",
    "            'label': torch.Tensor(single_label)\n",
    "        }\n",
    "        return batch\n",
    "    \n",
    "def get_TransferLearningDatasets(path, stim):\n",
    "    \n",
    "    # ==== GET PARTICIPANT FILENAME FOR EACH SPLIT ====\n",
    "    _, _, filenames = next(os.walk(path))\n",
    "    filenames = sorted(filenames)\n",
    "    torch.manual_seed(1)\n",
    "    num_par = len(filenames)\n",
    "    indices   = torch.randperm(num_par).tolist()\n",
    "    train_ind = int(0.85 * num_par)\n",
    "    val_ind   = int(0.95 * num_par)\n",
    "    train_par_idx = indices[ : train_ind] \n",
    "    val_par_idx   = indices[train_ind : val_ind] \n",
    "    test_par_idx  = indices[val_ind :]\n",
    "    train_par_filename = [filenames[train_par_id] for train_par_id in train_par_idx]\n",
    "    val_par_filename   = [filenames[val_par_id] for val_par_id in val_par_idx]\n",
    "    test_par_filename  = [filenames[test_par_id] for test_par_id in test_par_idx]\n",
    "    print(train_par_filename)\n",
    "    print(val_par_filename)\n",
    "    print(test_par_filename)\n",
    "    \n",
    "    # ==== USE FILENAME TO GET SEGMENTED DATASET ====\n",
    "    train_set = TransferLearningDataset(path, stim, train_par_filename) \n",
    "    val_set   = TransferLearningDataset(path, stim, val_par_filename)\n",
    "    test_set  = TransferLearningDataset(path, stim, test_par_filename)\n",
    "    \n",
    "    data_train  = train_set[:]['data']\n",
    "    label_train = train_set[:]['label']\n",
    "    data_val    = val_set[:]['data']\n",
    "    label_val   = val_set[:]['label']\n",
    "    data_test   = test_set[:]['data']\n",
    "    label_test  = test_set[:]['label']\n",
    "\n",
    "    print(\"Train Data shape : \" , data_train.shape)\n",
    "    print(\"Train Label shape: \" , label_train.shape) \n",
    "    print(\"val Data shape   : \" , data_val.shape) \n",
    "    print(\"val Label shape  : \" , label_val.shape) \n",
    "    print(\"test Data shape  : \" , data_test.shape)  \n",
    "    print(\"test Label shape : \" , label_test.shape) \n",
    "    \n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eec02af8-5fb4-4858-bffe-772694992f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s06.dat', 's27.dat', 's02.dat', 's11.dat', 's08.dat', 's25.dat', 's14.dat', 's24.dat', 's32.dat', 's20.dat', 's31.dat', 's18.dat', 's29.dat', 's09.dat', 's28.dat', 's30.dat', 's23.dat', 's05.dat', 's17.dat', 's22.dat', 's03.dat', 's16.dat', 's13.dat', 's04.dat', 's15.dat', 's12.dat', 's26.dat']\n",
      "['s10.dat', 's01.dat', 's21.dat']\n",
      "['s19.dat', 's07.dat']\n",
      "Train Data shape :  torch.Size([12960, 32, 672])\n",
      "Train Label shape:  torch.Size([12960, 1])\n",
      "val Data shape   :  torch.Size([1440, 32, 672])\n",
      "val Label shape  :  torch.Size([1440, 1])\n",
      "test Data shape  :  torch.Size([960, 32, 672])\n",
      "test Label shape :  torch.Size([960, 1])\n",
      "Train Data shape :  torch.Size([12960, 32, 672])\n",
      "Train Label shape:  torch.Size([12960, 1])\n",
      "val Data shape   :  torch.Size([1440, 32, 672])\n",
      "val Label shape  :  torch.Size([1440, 1])\n",
      "test Data shape  :  torch.Size([960, 32, 672])\n",
      "test Label shape :  torch.Size([960, 1])\n"
     ]
    }
   ],
   "source": [
    "path = \"data\" \n",
    "stim = \"Arousal\"\n",
    "\n",
    "train_set, val_set, test_set = get_TransferLearningDatasets(path, stim)\n",
    "\n",
    "data_train  = train_set[:]['data']\n",
    "label_train = train_set[:]['label']\n",
    "data_val    = val_set[:]['data']\n",
    "label_val   = val_set[:]['label']\n",
    "data_test   = test_set[:]['data']\n",
    "label_test  = test_set[:]['label']\n",
    "\n",
    "print(\"Train Data shape : \" , data_train.shape)\n",
    "print(\"Train Label shape: \" , label_train.shape) \n",
    "print(\"val Data shape   : \" , data_val.shape) \n",
    "print(\"val Label shape  : \" , label_val.shape) \n",
    "print(\"test Data shape  : \" , data_test.shape)  \n",
    "print(\"test Label shape : \" , label_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157a033f-02a5-4bdc-b023-a2fe9b93b4a4",
   "metadata": {},
   "source": [
    "### 2.2 ) Pool Data : Split video then do Segmentation\n",
    "\n",
    "- Combine data of all participant\n",
    "- Segments from one video will only be in one Split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5830b4c0-0e82-4ad5-a404-15146f7045f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SplitSegPoolDataset(path, stim):\n",
    "       \n",
    "    # ==== GET ALL DATA FROM ALL PAR ====\n",
    "    _, _, filenames = next(os.walk(path))\n",
    "    filenames = sorted(filenames)\n",
    "    all_data  = []\n",
    "    all_label = []\n",
    "    for dat in filenames:\n",
    "        temp = pickle.load(open(os.path.join(path, dat), 'rb'), encoding='latin1')\n",
    "        all_data.append(temp['data'])\n",
    "        if stim == \"Valence\":\n",
    "            all_label.append(temp['labels'][:,:1])  # first index is valence\n",
    "        elif stim == \"Arousal\":\n",
    "            all_label.append(temp['labels'][:,1:2]) # second index is arousal\n",
    "    all_data  = np.vstack(all_data)[:, :32, ]   # shape: (1280, 32, 8064) --> take only the first 32 channels\n",
    "    all_label = np.vstack(all_label)           # (1280, 1)  ==> 1280 samples, \n",
    "        \n",
    "    # ==== SPLIT DATA AT VIDEO LEVEL ====\n",
    "    len_all_ds = len(all_data)\n",
    "    torch.manual_seed(1)\n",
    "    indices   = torch.randperm(len_all_ds).tolist()\n",
    "    train_ind = int(0.85 * len_all_ds)\n",
    "    val_ind   = int(0.95 * len_all_ds)\n",
    "    train_idx = indices[ : train_ind] \n",
    "    val_idx   = indices[train_ind : val_ind] \n",
    "    test_idx  = indices[val_ind :]\n",
    "    \n",
    "    # ==== GET SEGMENTED DATASET ====\n",
    "    train_set = SegDataset(all_data, all_label, train_idx) \n",
    "    val_set   = SegDataset(all_data, all_label, val_idx)\n",
    "    test_set  = SegDataset(all_data, all_label, test_idx)\n",
    "    del all_data, all_label\n",
    "    \n",
    "    data_train  = train_set[:]['data']\n",
    "    label_train = train_set[:]['label']\n",
    "    data_val    = val_set[:]['data']\n",
    "    label_val   = val_set[:]['label']\n",
    "    data_test   = test_set[:]['data']\n",
    "    label_test  = test_set[:]['label']\n",
    "\n",
    "    print(\"Train Data shape : \" , data_train.shape)\n",
    "    print(\"Train Label shape: \" , label_train.shape) \n",
    "    print(\"val Data shape   : \" , data_val.shape) \n",
    "    print(\"val Label shape  : \" , label_val.shape) \n",
    "    print(\"test Data shape  : \" , data_test.shape)  \n",
    "    print(\"test Label shape : \" , label_test.shape) \n",
    "    \n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d9f2458-b2e0-44c6-a8a0-ca7a2a03a8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data shape :  torch.Size([13056, 32, 672])\n",
      "Train Label shape:  torch.Size([13056, 1])\n",
      "val Data shape   :  torch.Size([1536, 32, 672])\n",
      "val Label shape  :  torch.Size([1536, 1])\n",
      "test Data shape  :  torch.Size([768, 32, 672])\n",
      "test Label shape :  torch.Size([768, 1])\n"
     ]
    }
   ],
   "source": [
    "path = \"data\" \n",
    "stim = \"Arousal\"\n",
    "\n",
    "train_set, val_set, test_set = get_SplitSegPoolDataset(path, stim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b06a0a1-eecf-49ed-b39d-57f9c9814ece",
   "metadata": {},
   "source": [
    "### 2.3) Pool Data : Segment data then do Split\n",
    "\n",
    "- Combine data of all participant\n",
    "- Segments from one video can be in Train/Val/Test Split at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9db19573-35a7-4f2d-9407-61160a8a30b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SegSplitPoolDatasets(path, stim):\n",
    "    \n",
    "    # ==== GET ALL DATA FROM ALL PAR ====\n",
    "    _, _, filenames = next(os.walk(path))\n",
    "    filenames = sorted(filenames)\n",
    "    all_data  = []\n",
    "    all_label = []\n",
    "    for dat in filenames:\n",
    "        temp = pickle.load(open(os.path.join(path, dat), 'rb'), encoding='latin1')\n",
    "        all_data.append(temp['data'])\n",
    "        if stim == \"Valence\":\n",
    "            all_label.append(temp['labels'][:,:1])  # first index is valence\n",
    "        elif stim == \"Arousal\":\n",
    "            all_label.append(temp['labels'][:,1:2]) # second index is arousal\n",
    "    all_data  = np.vstack(all_data)[:, :32, ]   # shape: (1280, 32, 8064) --> take only the first 32 channels\n",
    "    all_label = np.vstack(all_label)           # (1280, 1)  ==> 1280 samples, \n",
    "         \n",
    "    # ==== DO SEGMENTATION ====\n",
    "    shape = all_data.shape\n",
    "    segments = 12\n",
    "    all_data = all_data.reshape(shape[0], shape[1], int(shape[2]/segments), segments)\n",
    "    #train data shape: (896, 32, 672, 12)\n",
    "    all_data = all_data.transpose(0, 3, 1, 2)\n",
    "    #train data shape: (896, 12, 32, 672)\n",
    "    all_data = all_data.reshape(shape[0] * segments, shape[1], -1)\n",
    "    #train data shape: (896*12, 32, 672)\n",
    "    all_label = np.repeat(all_label, segments)[:, np.newaxis]  #the dimension 1 is lost after repeat, so need to unsqueeze (896*12, 1)\n",
    "    \n",
    "    # ==== SPLIT SEGMENTED DATA ====\n",
    "    len_all_ds = len(all_data)\n",
    "    torch.manual_seed(1)\n",
    "    indices   = torch.randperm(len_all_ds).tolist()\n",
    "    train_ind = int(0.85 * len_all_ds)\n",
    "    val_ind   = int(0.95 * len_all_ds)\n",
    "    train_idx = indices[ : train_ind] \n",
    "    val_idx   = indices[train_ind : val_ind] \n",
    "    test_idx  = indices[val_ind :]\n",
    "    \n",
    "    # ==== GET SPLIT DATASET ====\n",
    "    train_set = SplitDataset(all_data, all_label, train_idx) \n",
    "    val_set   = SplitDataset(all_data, all_label, val_idx)\n",
    "    test_set  = SplitDataset(all_data, all_label, test_idx)\n",
    "    del all_data, all_label\n",
    "    \n",
    "    data_train  = train_set[:]['data']\n",
    "    label_train = train_set[:]['label']\n",
    "    data_val    = val_set[:]['data']\n",
    "    label_val   = val_set[:]['label']\n",
    "    data_test   = test_set[:]['data']\n",
    "    label_test  = test_set[:]['label']\n",
    "\n",
    "    print(\"Train Data shape : \" , data_train.shape)\n",
    "    print(\"Train Label shape: \" , label_train.shape) \n",
    "    print(\"val Data shape   : \" , data_val.shape) \n",
    "    print(\"val Label shape  : \" , label_val.shape) \n",
    "    print(\"test Data shape  : \" , data_test.shape)  \n",
    "    print(\"test Label shape : \" , label_test.shape) \n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac3d54da-b525-4967-a25c-e4ed5728fa9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data shape :  torch.Size([13056, 32, 672])\n",
      "Train Label shape:  torch.Size([13056, 1])\n",
      "val Data shape   :  torch.Size([1536, 32, 672])\n",
      "val Label shape  :  torch.Size([1536, 1])\n",
      "test Data shape  :  torch.Size([768, 32, 672])\n",
      "test Label shape :  torch.Size([768, 1])\n"
     ]
    }
   ],
   "source": [
    "path = \"data\" \n",
    "stim = \"Arousal\"\n",
    "\n",
    "train_set, val_set, test_set = get_SegSplitPoolDatasets(path, stim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4741a733-4843-4365-a02f-1970812400d2",
   "metadata": {},
   "source": [
    "## 3) Non-Segmented Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9298b84d-3c20-43e0-89c9-8ad3b5bc033e",
   "metadata": {},
   "source": [
    "### 3.1) Non-Segmented Data : from ONE participant\n",
    "\n",
    "- get data from only one participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7bab1c6-920a-4a1e-b530-1e88d72da927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NonSegmentOneParDatasets(path, stim, par, truncate = False):\n",
    "    \n",
    "    # ==== GET FILENAME OF THE PARTICIPANT THAT WE WANT ====\n",
    "    _, _, filenames = next(os.walk(path))\n",
    "    filenames = sorted(filenames)\n",
    "    par_filename = filenames[par]\n",
    "    print(par_filename)\n",
    "    \n",
    "    # ==== GET ALL DATA OF THAT PARTICIPANT ====\n",
    "    all_data  = []\n",
    "    all_label = []\n",
    "    temp = pickle.load(open(os.path.join(path, par_filename), 'rb'), encoding='latin1')\n",
    "    all_data.append(temp['data'])\n",
    "    if stim == \"Valence\":\n",
    "        all_label.append(temp['labels'][:,:1])  # first index is valence\n",
    "    elif stim == \"Arousal\":\n",
    "        all_label.append(temp['labels'][:,1:2]) # second index is arousal\n",
    "    if truncate != False :\n",
    "        all_data  = np.vstack(all_data)[:, :32, : truncate]\n",
    "    if truncate == False :\n",
    "        all_data  = np.vstack(all_data)[:, :32, ]   \n",
    "    all_label = np.vstack(all_label) \n",
    "    all_label = np.vstack(all_label)           # (1280, 1)  ==> 1280 samples, \n",
    "    \n",
    "    # ==== SPLIT DATA ====\n",
    "    len_all_ds = len(all_data)\n",
    "    torch.manual_seed(1)\n",
    "    indices   = torch.randperm(len_all_ds).tolist()\n",
    "    train_ind = int(0.85 * len_all_ds)\n",
    "    val_ind   = int(0.95 * len_all_ds)\n",
    "    train_idx = indices[ : train_ind] \n",
    "    val_idx   = indices[train_ind : val_ind] \n",
    "    test_idx  = indices[val_ind :]\n",
    "    \n",
    "    # ==== GET DATASET OF THAT PARTICIPANT ====\n",
    "    train_set = SplitDataset(all_data, all_label, train_idx) \n",
    "    val_set   = SplitDataset(all_data, all_label, val_idx)\n",
    "    test_set  = SplitDataset(all_data, all_label, test_idx)\n",
    "    del all_data, all_label\n",
    "    \n",
    "    data_train  = train_set[:]['data']\n",
    "    label_train = train_set[:]['label']\n",
    "    data_val    = val_set[:]['data']\n",
    "    label_val   = val_set[:]['label']\n",
    "    data_test   = test_set[:]['data']\n",
    "    label_test  = test_set[:]['label']\n",
    "\n",
    "    print(\"Train Data shape : \" , data_train.shape)\n",
    "    print(\"Train Label shape: \" , label_train.shape) \n",
    "    print(\"val Data shape   : \" , data_val.shape) \n",
    "    print(\"val Label shape  : \" , label_val.shape) \n",
    "    print(\"test Data shape  : \" , data_test.shape)  \n",
    "    print(\"test Label shape : \" , label_test.shape) \n",
    "    \n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a82e37a-5c6f-4a24-a8ac-ba7e36f8acd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s06.dat\n",
      "Train Data shape :  torch.Size([34, 32, 8064])\n",
      "Train Label shape:  torch.Size([34, 1])\n",
      "val Data shape   :  torch.Size([4, 32, 8064])\n",
      "val Label shape  :  torch.Size([4, 1])\n",
      "test Data shape  :  torch.Size([2, 32, 8064])\n",
      "test Label shape :  torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "path = \"data\" \n",
    "stim = \"Arousal\"\n",
    "par  = 5\n",
    "truncate = False\n",
    "\n",
    "train_set, val_set, test_set = get_NonSegmentOneParDatasets(path, stim, par, truncate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1911975c-f5cd-43aa-bbd0-be23f5c55ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s06.dat\n",
      "Train Data shape :  torch.Size([34, 32, 100])\n",
      "Train Label shape:  torch.Size([34, 1])\n",
      "val Data shape   :  torch.Size([4, 32, 100])\n",
      "val Label shape  :  torch.Size([4, 1])\n",
      "test Data shape  :  torch.Size([2, 32, 100])\n",
      "test Label shape :  torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "path = \"data\" \n",
    "stim = \"Arousal\"\n",
    "par  = 5\n",
    "truncate = 100\n",
    "\n",
    "train_set, val_set, test_set = get_NonSegmentOneParDatasets(path, stim, par, truncate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bc9121-1e8f-4d99-b903-fb3fb13d9a9f",
   "metadata": {},
   "source": [
    "## 3.2) Non-Segmented Data : from ALL participant\n",
    "- get data from ALL participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "474ff35f-60fd-426c-a54e-476f6ad91b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NonSegmentAllParDatasets(path, stim, truncate = False):\n",
    "    \n",
    "    # ==== GET ALL DATA FROM ALL PAR ====\n",
    "    _, _, filenames = next(os.walk(path))\n",
    "    filenames = sorted(filenames)\n",
    "    all_data  = []\n",
    "    all_label = []\n",
    "    for dat in filenames:\n",
    "        temp = pickle.load(open(os.path.join(path, dat), 'rb'), encoding='latin1')\n",
    "        all_data.append(temp['data'])\n",
    "        if stim == \"Valence\":\n",
    "            all_label.append(temp['labels'][:,:1])  # first index is valence\n",
    "        elif stim == \"Arousal\":\n",
    "            all_label.append(temp['labels'][:,1:2]) # second index is arousal\n",
    "            \n",
    "    if truncate != False :\n",
    "        all_data  = np.vstack(all_data)[:, :32, : truncate]\n",
    "    if truncate == False :\n",
    "        all_data  = np.vstack(all_data)[:, :32, ]   \n",
    "    all_label = np.vstack(all_label)          \n",
    "    \n",
    "    # ==== SPLIT DATA ====\n",
    "    len_all_ds = len(all_data)\n",
    "    torch.manual_seed(1)\n",
    "    indices   = torch.randperm(len_all_ds).tolist()\n",
    "    train_ind = int(0.85 * len_all_ds)\n",
    "    val_ind   = int(0.95 * len_all_ds)\n",
    "    train_idx = indices[ : train_ind] \n",
    "    val_idx   = indices[train_ind : val_ind] \n",
    "    test_idx  = indices[val_ind :]\n",
    "    \n",
    "    # ==== GET SPLIT DATASET ====\n",
    "    train_set = SplitDataset(all_data, all_label, train_idx) \n",
    "    val_set   = SplitDataset(all_data, all_label, val_idx)\n",
    "    test_set  = SplitDataset(all_data, all_label, test_idx)\n",
    "    del all_data, all_label\n",
    "    \n",
    "    data_train  = train_set[:]['data']\n",
    "    label_train = train_set[:]['label']\n",
    "    data_val    = val_set[:]['data']\n",
    "    label_val   = val_set[:]['label']\n",
    "    data_test   = test_set[:]['data']\n",
    "    label_test  = test_set[:]['label']\n",
    "\n",
    "    print(\"Train Data shape : \" , data_train.shape)\n",
    "    print(\"Train Label shape: \" , label_train.shape) \n",
    "    print(\"val Data shape   : \" , data_val.shape) \n",
    "    print(\"val Label shape  : \" , label_val.shape) \n",
    "    print(\"test Data shape  : \" , data_test.shape)  \n",
    "    print(\"test Label shape : \" , label_test.shape) \n",
    "    \n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "808ffc89-62bd-406e-ab13-9fc21b800630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data shape :  torch.Size([1088, 32, 8064])\n",
      "Train Label shape:  torch.Size([1088, 1])\n",
      "val Data shape   :  torch.Size([128, 32, 8064])\n",
      "val Label shape  :  torch.Size([128, 1])\n",
      "test Data shape  :  torch.Size([64, 32, 8064])\n",
      "test Label shape :  torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "path = \"data\" \n",
    "stim = \"Arousal\"\n",
    "truncate = False\n",
    "\n",
    "train_set, val_set, test_set = get_NonSegmentAllParDatasets(path, stim, truncate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9b8a730-ec37-4a98-8760-94b1cd574778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data shape :  torch.Size([1088, 32, 100])\n",
      "Train Label shape:  torch.Size([1088, 1])\n",
      "val Data shape   :  torch.Size([128, 32, 100])\n",
      "val Label shape  :  torch.Size([128, 1])\n",
      "test Data shape  :  torch.Size([64, 32, 100])\n",
      "test Label shape :  torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "path = \"data\" \n",
    "stim = \"Arousal\"\n",
    "truncate = 100\n",
    "\n",
    "train_set, val_set, test_set = get_NonSegmentAllParDatasets(path, stim, truncate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
       