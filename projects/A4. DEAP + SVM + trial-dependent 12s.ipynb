{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from components.dataset_jo import *\n",
    "\n",
    "dataset = Dataset_subjectDependent('data')\n",
    "segment_lenght = 12 #second\n",
    "dataset.set_segment(7680//(128*segment_lenght))\n",
    "for filename in dataset.get_file_list():\n",
    "    data, labels, _ = dataset.get_data(filename, return_type='numpy')\n",
    "    print(filename, labels.shape, labels.sum(axis=0)/labels.shape[0]*100)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_ori,y_ori,groups_ori,filename=None, kernel='rbf',return_text=False):\n",
    "    # Make a copy because I am paranoid\n",
    "    X,y,groups = X_ori.copy(), y_ori.copy(), groups_ori.copy()\n",
    "\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit \n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    model = SVC(kernel=kernel,max_iter=50000)\n",
    "    cv = StratifiedShuffleSplit(n_splits=10, train_size=0.75, random_state=0)\n",
    "    cross = cross_val_score(model, X, y, cv=cv, n_jobs=8)\n",
    "    \n",
    "    # We probably dont need this\n",
    "    model = SVC(kernel=kernel, max_iter=50000)\n",
    "    model.fit(X, y)\n",
    "    ans = model.predict(X)\n",
    "    acc = sum(ans == y) / len(y)\n",
    "    # If the model answer with all 0 or 1, we print this message\n",
    "    text = None\n",
    "    if( sum(ans) == len(y) or sum(ans) == 0 ): \n",
    "        text = f\"-----WARNING: Model {filename} failed to learn: sum(ans)={sum(ans)} sum(y)={sum(y)} len(y)={len(y)}\"\n",
    "    if(return_text):\n",
    "        return model, acc, cross, text\n",
    "    else:\n",
    "        print(text)\n",
    "        return model, acc, cross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spectral Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne_features.feature_extraction import FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [(0,4), (4,8), (8,12), (12,30), (30,64)]\n",
    "# [alias_feature_function]__[optional_param]\n",
    "params = dict({\n",
    "    'pow_freq_bands__log':True,\n",
    "    'pow_freq_bands__normalize':False,\n",
    "    'pow_freq_bands__freq_bands':bands\n",
    "})\n",
    "\n",
    "\n",
    "accs,cv_means,cv_stds  = [],[],[]\n",
    "# count = 0\n",
    "for filename in dataset.get_file_list():\n",
    "    start = time.time()\n",
    "    data, labels, groups = dataset.get_data(filename, stimuli=Dataset_subjectDependent.STIMULI_AROUSAL, return_type='numpy')\n",
    "    fe = FeatureExtractor(sfreq=128, selected_funcs=['pow_freq_bands'],params=params,n_jobs=8)\n",
    "    X = fe.fit_transform(X=data)\n",
    "    _, acc, cross = train_model(X, labels.reshape(-1), groups,filename=filename)\n",
    "    print(f\"\\tAROUSAL-{filename}|Acc={round(acc,5)}|10-CV={round(cross.mean(),5)}|STD={round(cross.std(),5)}|Time spend={time.time() - start}\")\n",
    "    accs.append(acc)\n",
    "    cv_means.append(cross.mean())\n",
    "    cv_stds.append(cross.std())\n",
    "    # count+=1\n",
    "    # if(count == 5): break\n",
    "print(f\"AROUSAL|Acc={sum(accs)/len(accs)}|10-CV={sum(cv_means)/len(cv_means)}|STD={sum(cv_stds)/len(cv_stds)}\")\n",
    "\n",
    "accs_v,cv_means_v,cv_stds_v  = [],[],[]\n",
    "# count=0\n",
    "for filename in dataset.get_file_list():\n",
    "    start = time.time()\n",
    "    data, labels, groups = dataset.get_data(filename, stimuli=Dataset_subjectDependent.STIMULI_VALENCE, return_type='numpy')\n",
    "    fe = FeatureExtractor(sfreq=128, selected_funcs=['pow_freq_bands'],params=params,n_jobs=8)\n",
    "    X = fe.fit_transform(X=data)\n",
    "    _, acc, cross = train_model(X, labels.reshape(-1), groups,filename=filename)\n",
    "    print(f\"\\tVALENCE-{filename}|Acc={round(acc,5)}|10-CV={round(cross.mean(),5)}|STD={round(cross.std(),5)}|Time spend={time.time() - start}\")\n",
    "    accs_v.append(acc)\n",
    "    cv_means_v.append(cross.mean())\n",
    "    cv_stds_v.append(cross.std())\n",
    "    # count+=1\n",
    "    # if(count == 5): break\n",
    "print(f\"VALENCE|Acc={sum(accs_v)/len(accs_v)}|10-CV={sum(cv_means_v)/len(cv_means_v)}|STD={sum(cv_stds_v)/len(cv_stds_v)}\")\n",
    "\n",
    "accs.extend(accs_v)\n",
    "cv_means.extend(cv_means_v)\n",
    "cv_stds.extend(cv_stds_v)\n",
    "print(f\"TOTAL|Acc={sum(accs)/len(accs)}|10-CV={sum(cv_means)/len(cv_means)}|STD={sum(cv_stds)/len(cv_stds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Asymetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = ['Fp1','AF3','F3','F7','FC5','FC1','C3','T7','CP5','CP1','P3','P7','PO3','O1','Oz','Pz','Fp2','AF4','Fz','F4','F8','FC6','FC2','Cz','C4','T8','CP6','CP2','P4','P8','PO4','O2']\n",
    "left_channels = ['Fp1','F7','F3','T7','P7','C3','P3','O1','AF3','FC5','FC1','CP5','CP1','PO3']\n",
    "right_channels = ['Fp2','F8','F4','T8','P8','C4','P4','O2','AF4','FC6','FC2','CP6','CP2','PO4']\n",
    "left_channel_indexes = [ channels.index(ch) for ch in left_channels ]\n",
    "right_channel_indexes = [ channels.index(ch) for ch in right_channels ]\n",
    "\n",
    "print(f\"{left_channel_indexes=}\")\n",
    "print(f\"{right_channel_indexes=}\")\n",
    "\n",
    "frontal_channels = ['FC5','FC1','FC2','FC6','F7','F3','Fz','F4','F8','Fp1','Fp2']\n",
    "posterior_channels = ['CP5','CP1','CP2','CP6','P7','P3','Pz','P4','P8','O1','O2']\n",
    "\n",
    "frontal_channel_indexes = [ channels.index(ch) for ch in frontal_channels ]\n",
    "posterior_channel_indexes = [ channels.index(ch) for ch in posterior_channels ]\n",
    "\n",
    "print(f\"{frontal_channel_indexes=}\")\n",
    "print(f\"{posterior_channel_indexes=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [(0,4), (4,8), (8,12), (12,30), (30,64)]\n",
    "# [alias_feature_function]__[optional_param]\n",
    "params = dict({\n",
    "    'pow_freq_bands__log':True,\n",
    "    'pow_freq_bands__normalize':False,\n",
    "    'pow_freq_bands__freq_bands':bands\n",
    "})\n",
    "\n",
    "accs,cv_means,cv_stds  = {'DASM':[],'RASM':[],'DCAU':[]},{'DASM':[],'RASM':[],'DCAU':[]},{'DASM':[],'RASM':[],'DCAU':[]}\n",
    "# count = 0\n",
    "for filename in dataset.get_file_list():\n",
    "    start = time.time()\n",
    "    data, labels, groups = dataset.get_data(filename, stimuli=Dataset_subjectDependent.STIMULI_AROUSAL, return_type='numpy')\n",
    "    fe = FeatureExtractor(sfreq=128, selected_funcs=['pow_freq_bands'],params=params,n_jobs=8)\n",
    "    X = fe.fit_transform(X=data)\n",
    "\n",
    "    PSD_left = X[:, left_channel_indexes].copy()\n",
    "    PSD_right = X[:, right_channel_indexes].copy()\n",
    "    PSD_frontal = X[:, frontal_channel_indexes].copy()\n",
    "    PSD_posterior = X[:, posterior_channel_indexes].copy()\n",
    "    X = {\n",
    "        'DASM': PSD_left - PSD_right,\n",
    "        'RASM': PSD_left / PSD_right,\n",
    "        'DCAU': PSD_frontal - PSD_posterior,\n",
    "    }\n",
    "\n",
    "    for feature_name in ['DASM','RASM','DCAU']:\n",
    "        _, acc, cross = train_model(X[feature_name], labels.reshape(-1), groups, filename=filename)\n",
    "        print(f\"\\tAROUSAL-{filename}-{feature_name}|Acc={round(acc,5)}|10-CV={round(cross.mean(),5)}|STD={round(cross.std(),5)}|Time spend={time.time() - start}\")\n",
    "        accs[feature_name].append(acc)\n",
    "        cv_means[feature_name].append(cross.mean())\n",
    "        cv_stds[feature_name].append(cross.std())\n",
    "    # count+=1\n",
    "    # if(count == 3): break\n",
    "for feature_name in ['DASM','RASM','DCAU']:\n",
    "    print(f\"AROUSAL-{feature_name}|Acc={sum(accs[feature_name])/len(accs[feature_name])}|10-CV={sum(cv_means[feature_name])/len(cv_means[feature_name])}|STD={sum(cv_stds[feature_name])/len(cv_stds[feature_name])}\")\n",
    "\n",
    "accs_v,cv_means_v,cv_stds_v  = {'DASM':[],'RASM':[],'DCAU':[]},{'DASM':[],'RASM':[],'DCAU':[]},{'DASM':[],'RASM':[],'DCAU':[]}\n",
    "# count=0\n",
    "for filename in dataset.get_file_list():\n",
    "    start = time.time()\n",
    "    data, labels, groups = dataset.get_data(filename, stimuli=Dataset_subjectDependent.STIMULI_VALENCE, return_type='numpy')\n",
    "    fe = FeatureExtractor(sfreq=128, selected_funcs=['pow_freq_bands'],params=params,n_jobs=8)\n",
    "    X = fe.fit_transform(X=data)\n",
    "\n",
    "    PSD_left = X[:, left_channel_indexes].copy()\n",
    "    PSD_right = X[:, right_channel_indexes].copy()\n",
    "    PSD_frontal = X[:, frontal_channel_indexes].copy()\n",
    "    PSD_posterior = X[:, posterior_channel_indexes].copy()\n",
    "    X = {\n",
    "        'DASM': PSD_left - PSD_right,\n",
    "        'RASM': PSD_left / PSD_right,\n",
    "        'DCAU': PSD_frontal - PSD_posterior,\n",
    "    }\n",
    "\n",
    "    for feature_name in ['DASM','RASM','DCAU']:\n",
    "        _, acc, cross = train_model(X[feature_name], labels.reshape(-1), groups, filename=filename)\n",
    "        print(f\"\\tVALENCE-{filename}-{feature_name}|Acc={round(acc,5)}|10-CV={round(cross.mean(),5)}|STD={round(cross.std(),5)}|Time spend={time.time() - start}\")\n",
    "        accs_v[feature_name].append(acc)\n",
    "        cv_means_v[feature_name].append(cross.mean())\n",
    "        cv_stds_v[feature_name].append(cross.std())\n",
    "    # count+=1\n",
    "    # if(count == 3): break\n",
    "for feature_name in ['DASM','RASM','DCAU']:\n",
    "    print(f\"VALENCE-{feature_name}|Acc={sum(accs_v[feature_name])/len(accs_v[feature_name])}|10-CV={sum(cv_means_v[feature_name])/len(cv_means_v[feature_name])}|STD={sum(cv_stds_v[feature_name])/len(cv_stds_v[feature_name])}\")\n",
    "\n",
    "for feature_name in ['DASM','RASM','DCAU']:\n",
    "    accs[feature_name].extend(accs_v[feature_name])\n",
    "    cv_means[feature_name].extend(cv_means_v[feature_name])\n",
    "    cv_stds[feature_name].extend(cv_stds_v[feature_name])\n",
    "    print(f\"TOTAL-{feature_name}|Acc={sum(accs[feature_name])/len(accs[feature_name])}|10-CV={sum(cv_means[feature_name])/len(cv_means[feature_name])}|STD={sum(cv_stds[feature_name])/len(cv_stds[feature_name])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Connectivity\n",
    "\n",
    "### 4.1 $ \\text{PCC}_{\\text{time}}(i,j) = \\frac{\\text{Cov}[\\mathbf{X}_i, \\mathbf{X}_j]}{\\sigma_{\\mathbf{X}_i} \\sigma_{\\mathbf{X}_j}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pearson_correlation(x,y):\n",
    "    \"\"\" x,y denoted the signal_x and signal_y following the equation \"\"\"\n",
    "    cov = np.cov(x, y)\n",
    "    # print(cov)\n",
    "    # [[ 8806859.74527069  8007149.0906219 ] ==> [[cov_xx, cov_xy]\n",
    "    # [ 8007149.0906219  10396797.72458848]]      [cov_yx, cov_yy]]\n",
    "    cov_xy = cov[0,1] # or cov[1,0]\n",
    "    cov_xx = cov[0,0]\n",
    "    cov_yy = cov[1,1]\n",
    "    corr = cov_xy / ( cov_xx**0.5 * cov_yy**0.5  )\n",
    "    return corr\n",
    "\n",
    "def _cal(p_id, partial_data):\n",
    "    # print(f\"p_id:{p_id} - data to run {partial_data.shape}\")\n",
    "    from itertools import combinations\n",
    "    pcc = []\n",
    "    for index in range(partial_data.shape[0]):\n",
    "        pcc_epoch = []\n",
    "        for comb in combinations(list(range(partial_data.shape[1])), 2):\n",
    "            pcc_ab = pearson_correlation(partial_data[index, comb[0], :], partial_data[index, comb[1], :]   )\n",
    "            pcc_epoch.append(pcc_ab)\n",
    "        pcc_epoch = np.hstack(pcc_epoch)\n",
    "        pcc.append(pcc_epoch)\n",
    "    pcc = np.vstack(pcc)\n",
    "    return pcc\n",
    "\n",
    "def calculate_pcc(data, n_jobs=8):\n",
    "    \"\"\" \n",
    "    Input: Expect data to have (n_epochs, n_channels, n_samples)\n",
    "    Output: (n_epochs, n_conn ) => n_conn = n_channels!/(2!(n_channels-2)!)\n",
    "    \"\"\"\n",
    "    from multiprocessing import Pool\n",
    "\n",
    "    try:\n",
    "        t_out = 60000\n",
    "        pool = Pool()\n",
    "        p_list = []\n",
    "        ans_list = []\n",
    "        num_p = n_jobs\n",
    "        indices = np.array_split(np.arange(data.shape[0]), num_p)\n",
    "        for p_id in range(num_p):\n",
    "            p_list.append(pool.apply_async(_cal, [p_id, data[indices[p_id]] ]))\n",
    "        for p_id in range(num_p):\n",
    "            ans_list.append( p_list[p_id].get(timeout=t_out) )\n",
    "        # ans_list\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        print(\"========= close ========\")\n",
    "        pool.close() \n",
    "        pool.terminate()\n",
    "    return np.vstack(ans_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs, cv_means, cv_stds = [],[],[]\n",
    "accs_v, cv_means_v, cv_stds_v = [],[],[]\n",
    "reports = []\n",
    "for filename in (pbar := tqdm(dataset.get_file_list())):\n",
    "    start = time.time()\n",
    "    pbar.set_description(filename)\n",
    "    data, labels, groups = dataset.get_data(filename, stimuli=Dataset_subjectDependent.STIMULI_AROUSAL, return_type='mne', sfreq=128)\n",
    "    data_csd = mne.preprocessing.compute_current_source_density(data)\n",
    "    pcc = calculate_pcc(data_csd.get_data())\n",
    "    _,acc,cross,train_report = train_model(pcc, labels.squeeze(), groups, filename=filename, return_text=True)\n",
    "    report = f\"\\tAROUSAL-{filename}|Acc={round(acc,5)}|10-CV={round(cross.mean(),5)}|STD={round(cross.std(),5)}|Time spend={time.time() - start}\"\n",
    "    print(report)\n",
    "    reports.append(report)\n",
    "    reports.append(train_report)\n",
    "    accs.append(acc)\n",
    "    cv_means.append(cross.mean())\n",
    "    cv_stds.append(cross.std())\n",
    "\n",
    "    _, labels_v, groups_v = dataset.get_data(filename, stimuli=Dataset_subjectDependent.STIMULI_VALENCE, return_type='mne', sfreq=128)\n",
    "    _,acc,cross,train_report = train_model(pcc, labels_v.squeeze(), groups_v, filename=filename, return_text=True)\n",
    "    report = f\"\\tVALENCE-{filename}|Acc={round(acc,5)}|10-CV={round(cross.mean(),5)}|STD={round(cross.std(),5)}|Time spend={time.time() - start}\"\n",
    "    print(report)\n",
    "    reports.append(report)\n",
    "    reports.append(train_report)\n",
    "    accs_v.append(acc)\n",
    "    cv_means_v.append(cross.mean())\n",
    "    cv_stds_v.append(cross.std())\n",
    "    # break\n",
    "    # count+=1\n",
    "    # if(count == 5): break\n",
    "report = f\"AROUSAL|Acc={sum(accs)/len(accs)}|10-CV={sum(cv_means)/len(cv_means)}|STD={sum(cv_stds)/len(cv_stds)}\"\n",
    "print(report)\n",
    "reports.append(report)\n",
    "report = f\"VALENCE|Acc={sum(accs_v)/len(accs_v)}|10-CV={sum(cv_means_v)/len(cv_means_v)}|STD={sum(cv_stds_v)/len(cv_stds_v)}\"\n",
    "print(report)\n",
    "reports.append(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in reports:\n",
    "    if(text == None): continue\n",
    "    print(text)\n",
    "\n",
    "accs.extend(accs_v)\n",
    "cv_means.extend(cv_means_v)\n",
    "cv_stds.extend(cv_stds_v)\n",
    "print(f\"TOTAL|Acc={sum(accs)/len(accs)}|10-CV={sum(cv_means)/len(cv_means)}|STD={sum(cv_stds)/len(cv_stds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 $ \\text{PCC}_{freq} (i,j) = \\frac{\\text{Cov}[\\hat{\\mathbf{X}}_i, \\hat{\\mathbf{X}}_j]}{\\sigma_{\\hat{\\mathbf{X}}_i} \\sigma_{\\hat{\\mathbf{X}}_j}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fft(signal, sfreq):\n",
    "    \"\"\" signal: can be 1D array of (n_sample,) or 2D array of (n_signal, n_sample)  \"\"\"\n",
    "    number_sample = signal.shape[-1]\n",
    "\n",
    "    # the result will be a complex number. We can obtain the magnitude using `absolute`\n",
    "    magnitude = np.abs(np.fft.fft(signal))\n",
    "    # scale the result\n",
    "    magnitude = magnitude / (number_sample/2)\n",
    "    # Selecting the range\n",
    "    magnitude = magnitude.T[:number_sample//2].T\n",
    "    freq_range = np.fft.fftfreq(number_sample, d=1/sfreq)[:number_sample//2]\n",
    "\n",
    "    return magnitude, freq_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs, cv_means, cv_stds = [],[],[]\n",
    "accs_v, cv_means_v, cv_stds_v = [],[],[]\n",
    "reports = []\n",
    "for filename in (pbar := tqdm(dataset.get_file_list())):\n",
    "    start = time.time()\n",
    "    pbar.set_description(filename)\n",
    "    data, labels, groups = dataset.get_data(filename, stimuli=Dataset_subjectDependent.STIMULI_AROUSAL, return_type='mne', sfreq=128)\n",
    "    \n",
    "    data_csd = mne.preprocessing.compute_current_source_density(data)\n",
    "    magnitude, freq_range = calculate_fft(data_csd.get_data(), 128)\n",
    "    pcc = calculate_pcc(magnitude)\n",
    "    _,acc,cross,train_report = train_model(pcc, labels.squeeze(), groups, filename=filename, return_text=True)\n",
    "    report = f\"\\tAROUSAL-{filename}|Acc={round(acc,5)}|10-CV={round(cross.mean(),5)}|STD={round(cross.std(),5)}|Time spend={time.time() - start}\"\n",
    "    print(report)\n",
    "    reports.append(report)\n",
    "    reports.append(train_report)\n",
    "    accs.append(acc)\n",
    "    cv_means.append(cross.mean())\n",
    "    cv_stds.append(cross.std())\n",
    "\n",
    "    _, labels_v, groups_v = dataset.get_data(filename, stimuli=Dataset_subjectDependent.STIMULI_VALENCE, return_type='mne', sfreq=128)\n",
    "    _,acc,cross,train_report = train_model(pcc, labels_v.squeeze(), groups_v, filename=filename, return_text=True)\n",
    "    report = f\"\\tVALENCE-{filename}|Acc={round(acc,5)}|10-CV={round(cross.mean(),5)}|STD={round(cross.std(),5)}|Time spend={time.time() - start}\"\n",
    "    print(report)\n",
    "    reports.append(report)\n",
    "    reports.append(train_report)\n",
    "    accs_v.append(acc)\n",
    "    cv_means_v.append(cross.mean())\n",
    "    cv_stds_v.append(cross.std())\n",
    "    # break\n",
    "    # count+=1\n",
    "    # if(count == 5): break\n",
    "report = f\"AROUSAL|Acc={sum(accs)/len(accs)}|10-CV={sum(cv_means)/len(cv_means)}|STD={sum(cv_stds)/len(cv_stds)}\"\n",
    "print(report)\n",
    "reports.append(report)\n",
    "report = f\"VALENCE|Acc={sum(accs_v)/len(accs_v)}|10-CV={sum(cv_means_v)/len(cv_means_v)}|STD={sum(cv_stds_v)/len(cv_stds_v)}\"\n",
    "print(report)\n",
    "reports.append(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in reports:\n",
    "    if(text == None): continue\n",
    "    print(text)\n",
    "\n",
    "accs.extend(accs_v)\n",
    "cv_means.extend(cv_means_v)\n",
    "cv_stds.extend(cv_stds_v)\n",
    "print(f\"TOTAL|Acc={sum(accs)/len(accs)}|10-CV={sum(cv_means)/len(cv_means)}|STD={sum(cv_stds)/len(cv_stds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 $ \\text{PLV}(j,k) = \\frac{1}{T} | \\Sigma^{T}_{t=1} e^{i(\\phi^{t}_{j} - \\phi^{t}_{k})}   | $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bands = [(0,4), (4,8), (8,12), (12,30), (30,64)]\n",
    "def calculate_stft(signals, sfreq):\n",
    "    from scipy import signal\n",
    "    f_range, t_range, Z = signal.stft(signals, sfreq, nperseg=sfreq//10, nfft=sfreq)\n",
    "    magnitude = np.abs(Z) \n",
    "    phase = np.angle(Z)\n",
    "    return magnitude, phase, f_range, t_range\n",
    "\n",
    "def PLV_stft(p_id, phase):\n",
    "    # (32, 65, 12) => (channels, freqs, times)\n",
    "    # print(f\"p_id:{p_id} - data to run {phase.shape}\")\n",
    "    print(phase.shape)\n",
    "    from itertools import combinations\n",
    "    plv = []\n",
    "    # count = 0\n",
    "    for index in range(phase.shape[0]):\n",
    "        plv_epoch = []\n",
    "        for comb in combinations(list(range(phase.shape[1])), 2):\n",
    "            # shape = (65,12)\n",
    "            phase_a, phase_b = phase[index, comb[0]], phase[index, comb[1]]\n",
    "            phase_diff = phase_a - phase_b\n",
    "            # sum along the time size\n",
    "            plv_ab = np.abs(np.average(np.exp(complex(0,1) * phase_diff), axis=1))\n",
    "            plv_epoch.append(plv_ab)\n",
    "        plv_epoch = np.vstack(plv_epoch)\n",
    "        # print(plv_epoch.shape) => (300, 32, 65, 23)\n",
    "        plv_epoch_5 = np.concatenate([ plv_epoch[:,0:4].mean(axis=1).reshape(-1,1),\n",
    "                                        plv_epoch[:,4:8].mean(axis=1).reshape(-1,1),\n",
    "                                        plv_epoch[:,8:12].mean(axis=1).reshape(-1,1),\n",
    "                                        plv_epoch[:,12:30].mean(axis=1).reshape(-1,1),\n",
    "                                        plv_epoch[:,30:65].mean(axis=1).reshape(-1,1)], axis=0)\n",
    "        plv.append(np.expand_dims(plv_epoch_5, axis=0))\n",
    "        # count += 1\n",
    "        # if(count == 3): break\n",
    "    # shape (496, 65)\n",
    "    # 496 is number of pairs that is not duplicate\n",
    "    # 65 is number of phase of frequencies\n",
    "    plv = np.vstack( plv )\n",
    "    return plv.squeeze()\n",
    "\n",
    "def calculate_plv(data, n_jobs=8):\n",
    "    \"\"\" \n",
    "    Input: Expect data to have (n_epochs, n_channels, n_samples)\n",
    "    Output: (n_epochs, n_conn, n_freqs ) => n_conn = n_channels!/(2!(n_channels-2)!)\n",
    "    \"\"\"\n",
    "    from multiprocessing import Pool\n",
    "\n",
    "    try:\n",
    "        t_out = 60000\n",
    "        pool = Pool()\n",
    "        p_list = []\n",
    "        ans_list = []\n",
    "        num_p = n_jobs\n",
    "        indices = np.array_split(np.arange(data.shape[0]), num_p)\n",
    "        _, phase, _, _ = calculate_stft(data, 128)\n",
    "        for p_id in range(num_p):\n",
    "            p_list.append(pool.apply_async(PLV_stft, [p_id, phase[indices[p_id]] ]))\n",
    "        for p_id in range(num_p):\n",
    "            ans_list.append( p_list[p_id].get(timeout=t_out) )\n",
    "        # ans_list\n",
    "    finally:\n",
    "        print(\"========= close ========\")\n",
    "        pool.close() \n",
    "        pool.terminate()\n",
    "    return np.vstack(ans_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs, cv_means, cv_stds = [],[],[]\n",
    "accs_v, cv_means_v, cv_stds_v = [],[],[]\n",
    "reports = []\n",
    "for filename in (pbar := tqdm(dataset.get_file_list())):\n",
    "    start = time.time()\n",
    "    pbar.set_description(filename)\n",
    "    data, labels, groups = dataset.get_data(filename, stimuli=Dataset_subjectDependent.STIMULI_AROUSAL, return_type='mne', sfreq=128)\n",
    "    \n",
    "    data_csd = mne.preprocessing.compute_current_source_density(data)\n",
    "\n",
    "    plv = calculate_plv(data_csd.get_data(), n_jobs=8)\n",
    "\n",
    "    _,acc,cross,train_report = train_model(plv, labels.squeeze(), groups, filename=filename, return_text=True)\n",
    "    report = f\"\\tAROUSAL-{filename}|Acc={round(acc,5)}|10-CV={round(cross.mean(),5)}|STD={round(cross.std(),5)}|Time spend={time.time() - start}\"\n",
    "    print(report)\n",
    "    reports.append(report)\n",
    "    reports.append(train_report)\n",
    "    accs.append(acc)\n",
    "    cv_means.append(cross.mean())\n",
    "    cv_stds.append(cross.std())\n",
    "\n",
    "    _, labels_v, groups_v = dataset.get_data(filename, stimuli=Dataset_subjectDependent.STIMULI_VALENCE, return_type='mne', sfreq=128)\n",
    "    _,acc,cross,train_report = train_model(plv, labels_v.squeeze(), groups_v, filename=filename, return_text=True)\n",
    "    report = f\"\\tVALENCE-{filename}|Acc={round(acc,5)}|10-CV={round(cross.mean(),5)}|STD={round(cross.std(),5)}|Time spend={time.time() - start}\"\n",
    "    print(report)\n",
    "    reports.append(report)\n",
    "    reports.append(train_report)\n",
    "    accs_v.append(acc)\n",
    "    cv_means_v.append(cross.mean())\n",
    "    cv_stds_v.append(cross.std())\n",
    "    # break\n",
    "    # count+=1\n",
    "    # if(count == 5): break\n",
    "report = f\"AROUSAL|Acc={sum(accs)/len(accs)}|10-CV={sum(cv_means)/len(cv_means)}|STD={sum(cv_stds)/len(cv_stds)}\"\n",
    "print(report)\n",
    "reports.append(report)\n",
    "report = f\"VALENCE|Acc={sum(accs_v)/len(accs_v)}|10-CV={sum(cv_means_v)/len(cv_means_v)}|STD={sum(cv_stds_v)/len(cv_stds_v)}\"\n",
    "print(report)\n",
    "reports.append(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in reports:\n",
    "    if(text == None): continue\n",
    "    print(text)\n",
    "\n",
    "accs.extend(accs_v)\n",
    "cv_means.extend(cv_means_v)\n",
    "cv_stds.extend(cv_stds_v)\n",
    "print(f\"TOTAL|Acc={sum(accs)/len(accs)}|10-CV={sum(cv_means)/len(cv_means)}|STD={sum(cv_stds)/len(cv_stds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 $ \\text{PLI}(j,k) =  \\frac{1}{T} | \\Sigma^{T}_{t=1} \\text{sign}(Im[e^{i (\\phi^{t}_{j} - \\phi^{t}_{k})}])  | $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bands = [(0,4), (4,8), (8,12), (12,30), (30,64)]\n",
    "def calculate_stft(signals, sfreq):\n",
    "    from scipy import signal\n",
    "    f_range, t_range, Z = signal.stft(signals, sfreq, nperseg=sfreq//10, nfft=sfreq)\n",
    "    magnitude = np.abs(Z) \n",
    "    phase = np.angle(Z)\n",
    "    return magnitude, phase, f_range, t_range\n",
    "\n",
    "def PLI_stft(p_id, phase):\n",
    "    # (32, 65, 12) => (channels, freqs, times)\n",
    "    # print(f\"p_id:{p_id} - data to run {phase.shape}\")\n",
    "    from itertools import combinations\n",
    "    pli = []\n",
    "    # count = 0\n",
    "    for index in range(phase.shape[0]):\n",
    "        pli_epoch = []\n",
    "        for comb in combinations(list(range(phase.shape[1])), 2):\n",
    "            # shape = (65,12)\n",
    "            phase_a, phase_b = phase[index, comb[0]], phase[index, comb[1]]\n",
    "            phase_diff = phase_a - phase_b\n",
    "            # sum along the time size\n",
    "            pli_ab = np.abs(np.average(   np.sign(np.imag(   np.exp(complex(0,1) * phase_diff) ))  , axis=1))\n",
    "            pli_epoch.append(pli_ab)\n",
    "        pli_epoch = np.vstack(pli_epoch)\n",
    "        pli_epoch_5 = np.concatenate([ pli_epoch[:,0:4].mean(axis=1).reshape(-1,1),\n",
    "                                        pli_epoch[:,4:8].mean(axis=1).reshape(-1,1),\n",
    "                                        pli_epoch[:,8:12].mean(axis=1).reshape(-1,1),\n",
    "                                        pli_epoch[:,12:30].mean(axis=1).reshape(-1,1),\n",
    "                                        pli_epoch[:,30:65].mean(axis=1).reshape(-1,1)], axis=0)\n",
    "        pli.append(np.expand_dims(pli_epoch_5, axis=0))\n",
    "        # count += 1\n",
    "        # if(count == 3): break\n",
    "    # shape (496, 65)\n",
    "    # 496 is number of pairs that is not duplicate\n",
    "    # 65 is number of phase of frequencies\n",
    "    pli = np.vstack( pli )\n",
    "    return pli.squeeze()\n",
    "\n",
    "def calculate_pli(data, n_jobs=8):\n",
    "    \"\"\" \n",
    "    Input: Expect data to have (n_epochs, n_channels, n_samples)\n",
    "    Output: (n_epochs, n_conn, n_freqs ) => n_conn = n_channels!/(2!(n_channels-2)!)\n",
    "    \"\"\"\n",
    "    from multiprocessing import Pool\n",
    "\n",
    "    try:\n",
    "        t_out = 60000\n",
    "        pool = Pool()\n",
    "        p_list = []\n",
    "        ans_list = []\n",
    "        num_p = n_jobs\n",
    "        indices = np.array_split(np.arange(data.shape[0]), num_p)\n",
    "        _, phase, _, _ = calculate_stft(data, 128)\n",
    "        for p_id in range(num_p):\n",
    "            p_list.append(pool.apply_async(PLI_stft, [p_id, phase[indices[p_id]] ]))\n",
    "        for p_id in range(num_p):\n",
    "            ans_list.append( p_list[p_id].get(timeout=t_out) )\n",
    "        # ans_list\n",
    "    finally:\n",
    "        print(\"========= close ========\")\n",
    "        pool.close() \n",
    "        pool.terminate()\n",
    "    return np.vstack(ans_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs, cv_means, cv_stds = [],[],[]\n",
    "accs_v, cv_means_v, cv_stds_v = [],[],[]\n",
    "reports = []\n",
    "for filename in (pbar := tqdm(dataset.get_file_list())):\n",
    "    start = time.time()\n",
    "    pbar.set_description(filename)\n",
    "    data, labels, groups = dataset.get_data(filename, stimuli=Dataset_subjectDependent.STIMULI_AROUSAL, return_type='mne', sfreq=128)\n",
    "    \n",
    "    data_csd = mne.preprocessing.compute_current_source_density(data)\n",
    "\n",
    "    pli = calculate_pli(data_csd.get_data(), n_jobs=8)\n",
    "\n",
    "    _,acc,cross,train_report = train_model(pli, labels.squeeze(), groups, filename=filename, return_text=True)\n",
    "    report = f\"\\tAROUSAL-{filename}|Acc={round(acc,5)}|10-CV={round(cross.mean(),5)}|STD={round(cross.std(),5)}|Time spend={time.time() - start}\"\n",
    "    print(report)\n",
    "    reports.append(report)\n",
    "    reports.append(train_report)\n",
    "    accs.append(acc)\n",
    "    cv_means.append(cross.mean())\n",
    "    cv_stds.append(cross.std())\n",
    "\n",
    "    _, labels_v, groups_v = dataset.get_data(filename, stimuli=Dataset_subjectDependent.STIMULI_VALENCE, return_type='mne', sfreq=128)\n",
    "    _,acc,cross,train_report = train_model(pli, labels_v.squeeze(), groups_v, filename=filename, return_text=True)\n",
    "    report = f\"\\tVALENCE-{filename}|Acc={round(acc,5)}|10-CV={round(cross.mean(),5)}|STD={round(cross.std(),5)}|Time spend={time.time() - start}\"\n",
    "    print(report)\n",
    "    reports.append(report)\n",
    "    reports.append(train_report)\n",
    "    accs_v.append(acc)\n",
    "    cv_means_v.append(cross.mean())\n",
    "    cv_stds_v.append(cross.std())\n",
    "    # break\n",
    "    # count+=1\n",
    "    # if(count == 5): break\n",
    "report = f\"AROUSAL|Acc={sum(accs)/len(accs)}|10-CV={sum(cv_means)/len(cv_means)}|STD={sum(cv_stds)/len(cv_stds)}\"\n",
    "print(report)\n",
    "reports.append(report)\n",
    "report = f\"VALENCE|Acc={sum(accs_v)/len(accs_v)}|10-CV={sum(cv_means_v)/len(cv_means_v)}|STD={sum(cv_stds_v)/len(cv_stds_v)}\"\n",
    "print(report)\n",
    "reports.append(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in reports:\n",
    "    if(text == None): continue\n",
    "    print(text)\n",
    "\n",
    "accs.extend(accs_v)\n",
    "cv_means.extend(cv_means_v)\n",
    "cv_stds.extend(cv_stds_v)\n",
    "print(f\"TOTAL|Acc={sum(accs)/len(accs)}|10-CV={sum(cv_means)/len(cv_means)}|STD={sum(cv_stds)/len(cv_stds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from components.CSP import CSP\n",
    "from mne_features.feature_extraction import FeatureExtractor\n",
    "\n",
    "bands = [(0,4), (4,8), (8,12), (12,30), (30,64)]\n",
    "# [alias_feature_function]__[optional_param]\n",
    "params = dict({\n",
    "    'pow_freq_bands__log':True,\n",
    "    'pow_freq_bands__normalize':False,\n",
    "    'pow_freq_bands__freq_bands':bands\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs, cv_means, cv_stds = [],[],[]\n",
    "for filename in tqdm(dataset.get_file_list()):\n",
    "\n",
    "    data, labels, groups = dataset.get_data(filename, stimuli=Dataset_subjectDependent.STIMULI_AROUSAL, return_type='numpy')\n",
    "    labels = labels.squeeze()\n",
    "    start = time.time()\n",
    "    filters = CSP(data[labels==0], data[labels==1])\n",
    "    \n",
    "    csp_data = []\n",
    "    for index in range(len(data)):\n",
    "        \n",
    "        # (672, 32) @ (32, 32) = (672, 32)\n",
    "        csp = data[index].T @ filters[labels[index].astype(int)]\n",
    "        # (1, 32, 672)\n",
    "        csp = np.expand_dims(csp.T, axis=0)\n",
    "        csp_data.append(csp)\n",
    "\n",
    "    csp_data = np.vstack(csp_data)\n",
    "    # print(csp_data.shape) => (2400, 32, 128)\n",
    "    fe = FeatureExtractor(sfreq=128, selected_funcs=['pow_freq_bands'],params=params,n_jobs=8)\n",
    "    X = fe.fit_transform(X=csp_data)\n",
    "    # print(X.shape, time.time() - start) => (2400, 160) 1.2374355792999268\n",
    "\n",
    "    assert X.shape == (2400,160)\n",
    "    _, acc, cross = train_model(X, labels.reshape(-1), groups, filename=filename)\n",
    "    print(f\"\\tAROUSAL-{filename}|Acc={round(acc,5)}|10-CV={round(cross.mean(),5)}|STD={round(cross.std(),5)}|Time spend={time.time() - start}\")\n",
    "    accs.append(acc)\n",
    "    cv_means.append(cross.mean())\n",
    "    cv_stds.append(cross.std())\n",
    "print(f\"AROUSAL|Acc={sum(accs)/len(accs)}|10-CV={sum(cv_means)/len(cv_means)}|STD={sum(cv_stds)/len(cv_stds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_v, cv_means_v, cv_stds_v = [],[],[]\n",
    "for filename in tqdm(dataset.get_file_list()):\n",
    "\n",
    "    data, labels, groups = dataset.get_data(filename, stimuli=Dataset_subjectDependent.STIMULI_VALENCE, return_type='numpy')\n",
    "    labels = labels.squeeze()\n",
    "    start = time.time()\n",
    "    filters = CSP(data[labels==0], data[labels==1])\n",
    "    \n",
    "    csp_data = []\n",
    "    for index in range(len(data)):\n",
    "        \n",
    "        # (672, 32) @ (32, 32) = (672, 32)\n",
    "        csp = data[index].T @ filters[labels[index].astype(int)]\n",
    "        # (1, 32, 672)\n",
    "        csp = np.expand_dims(csp.T, axis=0)\n",
    "        csp_data.append(csp)\n",
    "\n",
    "    csp_data = np.vstack(csp_data)\n",
    "    # print(csp_data.shape) => (2400, 32, 128)\n",
    "    fe = FeatureExtractor(sfreq=128, selected_funcs=['pow_freq_bands'],params=params,n_jobs=8)\n",
    "    X = fe.fit_transform(X=csp_data)\n",
    "    # print(X.shape, time.time() - start) => (2400, 160) 1.2374355792999268\n",
    "\n",
    "    assert X.shape == (2400,160)\n",
    "    _, acc, cross = train_model(X, labels.reshape(-1), groups, filename=filename)\n",
    "    print(f\"\\tVALENCE-{filename}|Acc={round(acc,5)}|10-CV={round(cross.mean(),5)}|STD={round(cross.std(),5)}|Time spend={time.time() - start}\")\n",
    "    accs_v.append(acc)\n",
    "    cv_means_v.append(cross.mean())\n",
    "    cv_stds_v.append(cross.std())\n",
    "print(f\"VALENCE|Acc={sum(accs_v)/len(accs_v)}|10-CV={sum(cv_means_v)/len(cv_means_v)}|STD={sum(cv_stds_v)/len(cv_stds_v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs.extend(accs_v)\n",
    "cv_means.extend(cv_means_v)\n",
    "cv_stds.extend(cv_stds_v)\n",
    "\n",
    "print(f\"TOTAL|Acc={sum(accs)/len(accs)}|10-CV={sum(cv_means)/len(cv_means)}|STD={sum(cv_stds)/len(cv_stds)}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27768773b483d82a9b2b839e3fa80b1be5789db7fd78df4eedef2df266871616"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
